{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4bb81b-a80c-4e9e-88a5-c51f23f34456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: eval_model_on_video/output_20241018_074844\n",
      "Number of missing 'overall_turn_label': 0\n",
      "Using device: cuda\n",
      "Total number of samples: 239560\n",
      "Number of batches per epoch: 234\n",
      "Starting epoch 1/10 for current model\n",
      "Epoch [1/10] for current model, Average Loss: 0.1051\n",
      "Starting epoch 2/10 for current model\n",
      "Epoch [2/10] for current model, Average Loss: 0.0057\n",
      "Starting epoch 3/10 for current model\n",
      "Epoch [3/10] for current model, Average Loss: 0.0040\n",
      "Starting epoch 4/10 for current model\n",
      "Epoch [4/10] for current model, Average Loss: 0.0032\n",
      "Starting epoch 5/10 for current model\n",
      "Epoch [5/10] for current model, Average Loss: 0.0029\n",
      "Starting epoch 6/10 for current model\n",
      "Epoch [6/10] for current model, Average Loss: 0.0018\n",
      "Starting epoch 7/10 for current model\n",
      "Epoch [7/10] for current model, Average Loss: 0.0014\n",
      "Starting epoch 8/10 for current model\n",
      "Epoch [8/10] for current model, Average Loss: 0.0012\n",
      "Starting epoch 9/10 for current model\n",
      "Epoch [9/10] for current model, Average Loss: 0.0010\n",
      "Starting epoch 10/10 for current model\n",
      "Epoch [10/10] for current model, Average Loss: 0.0009\n",
      "Total number of samples for baseline: 239560\n",
      "Number of batches per epoch for baseline: 234\n",
      "Starting epoch 1/10 for baseline model\n",
      "Epoch [1/10] for baseline model, Average Loss: 0.1189\n",
      "Starting epoch 2/10 for baseline model\n",
      "Epoch [2/10] for baseline model, Average Loss: 0.0045\n",
      "Starting epoch 3/10 for baseline model\n",
      "Epoch [3/10] for baseline model, Average Loss: 0.0029\n",
      "Starting epoch 4/10 for baseline model\n",
      "Epoch [4/10] for baseline model, Average Loss: 0.0024\n",
      "Starting epoch 5/10 for baseline model\n",
      "Epoch [5/10] for baseline model, Average Loss: 0.0020\n",
      "Starting epoch 6/10 for baseline model\n",
      "Epoch [6/10] for baseline model, Average Loss: 0.0018\n",
      "Starting epoch 7/10 for baseline model\n",
      "Epoch [7/10] for baseline model, Average Loss: 0.0016\n",
      "Starting epoch 8/10 for baseline model\n",
      "Epoch [8/10] for baseline model, Average Loss: 0.0015\n",
      "Starting epoch 9/10 for baseline model\n",
      "Epoch [9/10] for baseline model, Average Loss: 0.0012\n",
      "Starting epoch 10/10 for baseline model\n",
      "Epoch [10/10] for baseline model, Average Loss: 0.0010\n",
      "\n",
      "Metrics for current model at horizon: 0.5 seconds (15 frames)\n",
      "RMSE: 0.0252\n",
      "MAE: 0.0147\n",
      "ADE: 0.0234\n",
      "FDE: 0.0233\n",
      "\n",
      "Metrics for current model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0240\n",
      "MAE: 0.0160\n",
      "ADE: 0.0250\n",
      "FDE: 0.0319\n",
      "\n",
      "Metrics for current model at horizon: 1.5 seconds (45 frames)\n",
      "RMSE: 0.0298\n",
      "MAE: 0.0193\n",
      "ADE: 0.0303\n",
      "FDE: 0.0493\n",
      "\n",
      "Metrics for baseline model at horizon: 0.5 seconds (15 frames)\n",
      "RMSE: 0.0253\n",
      "MAE: 0.0145\n",
      "ADE: 0.0231\n",
      "FDE: 0.0229\n",
      "\n",
      "Metrics for baseline model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0241\n",
      "MAE: 0.0157\n",
      "ADE: 0.0250\n",
      "FDE: 0.0326\n",
      "\n",
      "Metrics for baseline model at horizon: 1.5 seconds (45 frames)\n",
      "RMSE: 0.0303\n",
      "MAE: 0.0193\n",
      "ADE: 0.0308\n",
      "FDE: 0.0514\n",
      "Current model saved to eval_model_on_video/output_20241018_074844/trajectory_predictor_current_20241018_074844.pth\n",
      "Current scaler saved to eval_model_on_video/output_20241018_074844/scaler_current_20241018_074844.save\n",
      "Baseline model saved to eval_model_on_video/output_20241018_074844/trajectory_predictor_baseline_20241018_074844.pth\n",
      "Baseline scaler saved to eval_model_on_video/output_20241018_074844/scaler_baseline_20241018_074844.save\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Current_Model_vehicle_185_seq_172706_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Current_Model_vehicle_220_seq_188381_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Current_Model_vehicle_278_seq_202552_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Current_Model_vehicle_328_seq_230159_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Current_Model_vehicle_93_seq_138782_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Baseline_Model_vehicle_185_seq_173087_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Baseline_Model_vehicle_278_seq_202260_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Baseline_Model_vehicle_278_seq_202360_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Baseline_Model_vehicle_278_seq_202669_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Baseline_Model_vehicle_238_seq_196797_20241018_074844.png\n",
      "Metrics comparison saved to eval_model_on_video/output_20241018_074844/metrics_comparison.csv\n",
      "\n",
      "Comparison of Performance Metrics:\n",
      "   Horizon (s)  Horizon (frames)     Model      RMSE       MAE       ADE  \\\n",
      "0          0.5                15  Baseline  0.025331  0.014471  0.023095   \n",
      "1          0.5                15   Current  0.025172  0.014721  0.023404   \n",
      "2          1.0                30  Baseline  0.024112  0.015657  0.024951   \n",
      "3          1.0                30   Current  0.023965  0.015956  0.024998   \n",
      "4          1.5                45  Baseline  0.030272  0.019332  0.030759   \n",
      "5          1.5                45   Current  0.029824  0.019296  0.030322   \n",
      "\n",
      "        FDE  \n",
      "0  0.022886  \n",
      "1  0.023341  \n",
      "2  0.032596  \n",
      "3  0.031891  \n",
      "4  0.051385  \n",
      "5  0.049344  \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "path = Path('csv_out')\n",
    "eval_video_path = Path('eval_model_on_video')\n",
    "\n",
    "# Create new output directory with timestamp to avoid overwriting\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = eval_video_path / f'output_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Outputs will be saved to: {output_dir}')\n",
    "\n",
    "# Load data\n",
    "df1 = pd.read_csv(path / 'tracking_data.csv')\n",
    "df2 = pd.read_csv(path / 'overall_turn_label.csv')\n",
    "\n",
    "# Merge dataframes\n",
    "df_merged = pd.merge(df1, df2[['id', 'frame', 'overall_turn_label']], on=['id', 'frame'], how='left')\n",
    "\n",
    "# Fill missing 'overall_turn_label' values by forward and backward filling per vehicle id\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='ffill')\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='bfill')\n",
    "\n",
    "# Check for remaining missing values\n",
    "missing_values = df_merged['overall_turn_label'].isnull().sum()\n",
    "print(f\"Number of missing 'overall_turn_label': {missing_values}\")\n",
    "\n",
    "# Fill any remaining missing values with 'straight'\n",
    "df_merged['overall_turn_label'] = df_merged['overall_turn_label'].fillna('straight')\n",
    "\n",
    "# One-Hot encode 'overall_turn_label'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turn_labels_encoded = encoder.fit_transform(df_merged[['overall_turn_label']])\n",
    "turn_label_columns = encoder.get_feature_names_out(['overall_turn_label'])\n",
    "df_merged[turn_label_columns] = turn_labels_encoded\n",
    "\n",
    "# Define input features\n",
    "input_features = ['center_x', 'center_y'] + list(turn_label_columns)\n",
    "\n",
    "# Define sequence lengths\n",
    "sequence_length = 90  # Input sequence length (90 frames, 3 seconds at 30 fps)\n",
    "predict_length = 45   # Prediction sequence length (45 frames, 1.5 seconds at 30 fps)\n",
    "\n",
    "# Generate input and target sequences for the current model (with turn feature)\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "sequence_vehicle_ids = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  # Only 'center_x' and 'center_y'\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "        sequence_vehicle_ids.append(track_id)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "sequence_vehicle_ids = np.array(sequence_vehicle_ids)\n",
    "\n",
    "# Data normalization\n",
    "numeric_feature_indices = [0, 1]  # Indices for 'center_x' and 'center_y'\n",
    "\n",
    "# Flatten the inputs for scaling\n",
    "all_numeric_inputs = input_sequences[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_numeric_inputs)\n",
    "\n",
    "# Scale inputs\n",
    "input_sequences[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs).reshape(\n",
    "    input_sequences.shape[0], input_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets\n",
    "all_numeric_targets = target_sequences.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences = scaler.transform(all_numeric_targets).reshape(\n",
    "    target_sequences.shape[0], target_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Prepare baseline data (without turn feature)\n",
    "input_features_baseline = ['center_x', 'center_y']\n",
    "\n",
    "input_sequences_baseline = []\n",
    "target_sequences_baseline = []\n",
    "sequence_vehicle_ids_baseline = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features_baseline].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]\n",
    "\n",
    "        input_sequences_baseline.append(input_seq)\n",
    "        target_sequences_baseline.append(target_seq)\n",
    "        sequence_vehicle_ids_baseline.append(track_id)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "input_sequences_baseline = np.array(input_sequences_baseline)\n",
    "target_sequences_baseline = np.array(target_sequences_baseline)\n",
    "sequence_vehicle_ids_baseline = np.array(sequence_vehicle_ids_baseline)\n",
    "\n",
    "# Data normalization for baseline\n",
    "all_numeric_inputs_baseline = input_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler_baseline = StandardScaler()\n",
    "scaler_baseline.fit(all_numeric_inputs_baseline)\n",
    "\n",
    "# Scale inputs\n",
    "input_sequences_baseline = scaler_baseline.transform(all_numeric_inputs_baseline).reshape(\n",
    "    input_sequences_baseline.shape[0], input_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets\n",
    "all_numeric_targets_baseline = target_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_baseline = scaler_baseline.transform(all_numeric_targets_baseline).reshape(\n",
    "    target_sequences_baseline.shape[0], target_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Define the model\n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=2):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        self.lstm_encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_decoder = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, target_len):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Encoder\n",
    "        _, (hidden, cell) = self.lstm_encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_input = x[:, -1, :2].unsqueeze(1)  # Start with the last position of the input sequence\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            out, (hidden, cell) = self.lstm_decoder(decoder_input, (hidden, cell))\n",
    "            out = self.fc_out(out)\n",
    "            outputs.append(out.squeeze(1))\n",
    "            decoder_input = out  # Use current output as next input\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 1024\n",
    "num_epochs = 10\n",
    "target_len = predict_length  # Prediction length\n",
    "\n",
    "# Prepare data for current model\n",
    "input_size = input_sequences.shape[2]  # Number of input features\n",
    "inputs = torch.tensor(input_sequences, dtype=torch.float32).to(device)\n",
    "targets = torch.tensor(target_sequences, dtype=torch.float32).to(device)\n",
    "model = TrajectoryPredictor(input_size=input_size, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "print(f'Number of batches per epoch: {len(data_loader)}')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for current model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for current model')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for current model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# Prepare data for baseline model\n",
    "input_size_baseline = input_sequences_baseline.shape[2]  # Number of input features (without turn feature)\n",
    "inputs_baseline = torch.tensor(input_sequences_baseline, dtype=torch.float32).to(device)\n",
    "targets_baseline = torch.tensor(target_sequences_baseline, dtype=torch.float32).to(device)\n",
    "model_baseline = TrajectoryPredictor(input_size=input_size_baseline, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset_baseline = TensorDataset(inputs_baseline, targets_baseline)\n",
    "data_loader_baseline = DataLoader(dataset_baseline, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples for baseline: {len(dataset_baseline)}')\n",
    "print(f'Number of batches per epoch for baseline: {len(data_loader_baseline)}')\n",
    "\n",
    "# Define loss function and optimizer for baseline model\n",
    "criterion_baseline = nn.MSELoss()\n",
    "optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for baseline model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for baseline model')\n",
    "    model_baseline.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer_baseline.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion_baseline(outputs, batch_targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer_baseline.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader_baseline)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for baseline model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# Define function to compute metrics\n",
    "def compute_metrics(predictions, targets, horizons):\n",
    "    metrics = {}\n",
    "    for horizon in horizons:\n",
    "        outputs_at_horizon = predictions[:, :horizon, :]  # [num_samples, horizon, 2]\n",
    "        targets_at_horizon = targets[:, :horizon, :]\n",
    "\n",
    "        # Compute errors\n",
    "        errors = outputs_at_horizon - targets_at_horizon  # [num_samples, horizon, 2]\n",
    "        squared_errors = errors ** 2\n",
    "        mse = squared_errors.mean().item()\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        abs_errors = errors.abs()\n",
    "        mae = abs_errors.mean().item()\n",
    "\n",
    "        # Compute ADE\n",
    "        displacement_errors = torch.norm(errors, dim=2)  # Euclidean distance over x and y\n",
    "        ade = displacement_errors.mean().item()\n",
    "\n",
    "        # Compute FDE\n",
    "        final_errors = errors[:, -1, :]  # [num_samples, 2]\n",
    "        fde = torch.norm(final_errors, dim=1).mean().item()\n",
    "\n",
    "        metrics[horizon] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'ADE': ade,\n",
    "            'FDE': fde\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Define prediction horizons\n",
    "horizons = {\n",
    "    15: 0.5,  # 0.5 seconds (15 frames)\n",
    "    30: 1.0,  # 1.0 seconds (30 frames)\n",
    "    45: 1.5   # 1.5 seconds (45 frames)\n",
    "}\n",
    "\n",
    "# Filter horizons not exceeding predict_length\n",
    "horizons = {k: v for k, v in horizons.items() if k <= predict_length}\n",
    "\n",
    "# Evaluate current model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs = []\n",
    "    total_targets = []\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "        total_outputs.append(outputs.cpu())\n",
    "        total_targets.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs = torch.cat(total_outputs, dim=0)\n",
    "    total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "    metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "\n",
    "    # Save metrics to file\n",
    "    metrics_file = output_dir / 'metrics_current_model.txt'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f'Metrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "            f.write(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# Evaluate baseline model\n",
    "model_baseline.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs_baseline = []\n",
    "    total_targets_baseline = []\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "        total_outputs_baseline.append(outputs.cpu())\n",
    "        total_targets_baseline.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs_baseline = torch.cat(total_outputs_baseline, dim=0)\n",
    "    total_targets_baseline = torch.cat(total_targets_baseline, dim=0)\n",
    "\n",
    "    metrics_baseline = compute_metrics(total_outputs_baseline, total_targets_baseline, horizons.keys())\n",
    "\n",
    "    # Save metrics to file\n",
    "    metrics_file_baseline = output_dir / 'metrics_baseline_model.txt'\n",
    "    with open(metrics_file_baseline, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            # Corrected the mismatched quotation mark\n",
    "            f.write(f\"Metrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n\")\n",
    "            f.write(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# Save models and scalers\n",
    "model_file = output_dir / f'trajectory_predictor_current_{timestamp}.pth'\n",
    "scaler_file = output_dir / f'scaler_current_{timestamp}.save'\n",
    "\n",
    "torch.save(model.state_dict(), model_file)\n",
    "joblib.dump(scaler, scaler_file)\n",
    "print(f'Current model saved to {model_file}')\n",
    "print(f'Current scaler saved to {scaler_file}')\n",
    "\n",
    "model_file_baseline = output_dir / f'trajectory_predictor_baseline_{timestamp}.pth'\n",
    "scaler_file_baseline = output_dir / f'scaler_baseline_{timestamp}.save'\n",
    "\n",
    "torch.save(model_baseline.state_dict(), model_file_baseline)\n",
    "joblib.dump(scaler_baseline, scaler_file_baseline)\n",
    "print(f'Baseline model saved to {model_file_baseline}')\n",
    "print(f'Baseline scaler saved to {scaler_file_baseline}')\n",
    "\n",
    "# Visualization (optional)\n",
    "vehicle_ids_of_interest = [50, 328, 220, 46, 201, 238, 278, 185, 309, 303, 74, 93, 127, 203, 219, 210, 280, 390]\n",
    "\n",
    "# Create vehicle ID to indices mapping for current model\n",
    "vehicle_id_to_indices_current = {}\n",
    "for vehicle_id in vehicle_ids_of_interest:\n",
    "    indices = np.where(sequence_vehicle_ids == vehicle_id)[0]\n",
    "    if len(indices) > 0:\n",
    "        vehicle_id_to_indices_current[vehicle_id] = indices\n",
    "    else:\n",
    "        print(f\"Vehicle ID {vehicle_id} not found in the current model sequences.\")\n",
    "\n",
    "# Create vehicle ID to indices mapping for baseline model\n",
    "vehicle_id_to_indices_baseline = {}\n",
    "for vehicle_id in vehicle_ids_of_interest:\n",
    "    indices = np.where(sequence_vehicle_ids_baseline == vehicle_id)[0]\n",
    "    if len(indices) > 0:\n",
    "        vehicle_id_to_indices_baseline[vehicle_id] = indices\n",
    "    else:\n",
    "        print(f\"Vehicle ID {vehicle_id} not found in the baseline model sequences.\")\n",
    "\n",
    "# Define visualization function\n",
    "def visualize_prediction(model, inputs, targets, scaler, vehicle_id_to_indices, model_name):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create index to vehicle ID mapping\n",
    "        index_to_vehicle_id = {}\n",
    "        for vehicle_id, indices in vehicle_id_to_indices.items():\n",
    "            for idx in indices:\n",
    "                index_to_vehicle_id[idx] = vehicle_id\n",
    "\n",
    "        # Collect all indices\n",
    "        all_indices = list(index_to_vehicle_id.keys())\n",
    "\n",
    "        # Randomly select 5 indices\n",
    "        num_samples = 5\n",
    "        if len(all_indices) >= num_samples:\n",
    "            selected_indices = random.sample(all_indices, num_samples)\n",
    "        else:\n",
    "            selected_indices = all_indices  # If less than 5 sequences are available\n",
    "\n",
    "        for idx in selected_indices:\n",
    "            test_input = inputs[idx].unsqueeze(0).to(device)\n",
    "            true_target = targets[idx].to(device)\n",
    "\n",
    "            # Perform prediction\n",
    "            predicted_output = model(test_input, target_len)\n",
    "\n",
    "            # Convert predictions and true targets to NumPy arrays\n",
    "            predicted_output = predicted_output.squeeze(0).cpu().numpy()\n",
    "            true_target = true_target.cpu().numpy()\n",
    "\n",
    "            # Get historical input data for visualization\n",
    "            history_input = test_input.squeeze(0).cpu().numpy()\n",
    "\n",
    "            # Inverse scaling\n",
    "            numeric_feature_indices = [0, 1]  # Indices of 'center_x' and 'center_y'\n",
    "\n",
    "            # Inverse transform historical inputs\n",
    "            history_input_numeric = history_input[:, numeric_feature_indices]\n",
    "            history_input_unscaled = scaler.inverse_transform(history_input_numeric)\n",
    "\n",
    "            # Inverse transform predicted outputs\n",
    "            predicted_output_unscaled = scaler.inverse_transform(predicted_output)\n",
    "\n",
    "            # Inverse transform true targets\n",
    "            true_target_unscaled = scaler.inverse_transform(true_target)\n",
    "\n",
    "            # Visualization\n",
    "            plt.figure(figsize=(8, 6))\n",
    "\n",
    "            # Plot historical trajectory\n",
    "            plt.plot(history_input_unscaled[:, 0], history_input_unscaled[:, 1], 'bo-', label='Historical Trajectory')\n",
    "\n",
    "            # Plot true future trajectory\n",
    "            plt.plot(true_target_unscaled[:, 0], true_target_unscaled[:, 1], 'go-', label='True Future Trajectory')\n",
    "\n",
    "            # Plot predicted future trajectory\n",
    "            plt.plot(predicted_output_unscaled[:, 0], predicted_output_unscaled[:, 1], 'ro--', label='Predicted Future Trajectory')\n",
    "\n",
    "            plt.legend()\n",
    "            plt.xlabel('center_x')\n",
    "            plt.ylabel('center_y')\n",
    "            vehicle_id = index_to_vehicle_id.get(idx, 'Unknown')\n",
    "            plt.title(f'{model_name} - Vehicle {vehicle_id} Trajectory Prediction (Seq {idx})')\n",
    "\n",
    "            # Save figure\n",
    "            figure_path = output_dir / f'{model_name}_vehicle_{vehicle_id}_seq_{idx}_{timestamp}.png'\n",
    "            plt.savefig(figure_path)\n",
    "            plt.close()\n",
    "            print(f'Plot saved to {figure_path}')\n",
    "\n",
    "# Visualize predictions for current model\n",
    "visualize_prediction(model, inputs.cpu(), targets.cpu(), scaler, vehicle_id_to_indices_current, model_name='Current_Model')\n",
    "\n",
    "# Visualize predictions for baseline model\n",
    "visualize_prediction(model_baseline, inputs_baseline.cpu(), targets_baseline.cpu(), scaler_baseline, vehicle_id_to_indices_baseline, model_name='Baseline_Model')\n",
    "\n",
    "# Combine metrics from both models into a DataFrame and save\n",
    "metrics_combined = []\n",
    "\n",
    "for horizon_frames, time_sec in horizons.items():\n",
    "    metrics_combined.append({\n",
    "        'Horizon (s)': time_sec,\n",
    "        'Horizon (frames)': horizon_frames,\n",
    "        'Model': 'Baseline',\n",
    "        'RMSE': metrics_baseline[horizon_frames]['RMSE'],\n",
    "        'MAE': metrics_baseline[horizon_frames]['MAE'],\n",
    "        'ADE': metrics_baseline[horizon_frames]['ADE'],\n",
    "        'FDE': metrics_baseline[horizon_frames]['FDE']\n",
    "    })\n",
    "    metrics_combined.append({\n",
    "        'Horizon (s)': time_sec,\n",
    "        'Horizon (frames)': horizon_frames,\n",
    "        'Model': 'Current',\n",
    "        'RMSE': metrics[horizon_frames]['RMSE'],\n",
    "        'MAE': metrics[horizon_frames]['MAE'],\n",
    "        'ADE': metrics[horizon_frames]['ADE'],\n",
    "        'FDE': metrics[horizon_frames]['FDE']\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_combined)\n",
    "metrics_csv_file = output_dir / 'metrics_comparison.csv'\n",
    "df_metrics.to_csv(metrics_csv_file, index=False)\n",
    "print(f'Metrics comparison saved to {metrics_csv_file}')\n",
    "\n",
    "# Print comparison of performance metrics\n",
    "print(\"\\nComparison of Performance Metrics:\")\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6485810-8dbc-4663-851f-e3cba773b579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: eval_model_on_video/output_20241018_080535\n",
      "Number of missing 'overall_turn_label': 0\n",
      "Using device: cuda\n",
      "Total number of samples: 239560\n",
      "Number of batches per epoch: 234\n",
      "Starting epoch 1/10 for current model\n",
      "Epoch [1/10] for current model, Average Loss: 0.1196\n",
      "Starting epoch 2/10 for current model\n",
      "Epoch [2/10] for current model, Average Loss: 0.0049\n",
      "Starting epoch 3/10 for current model\n",
      "Epoch [3/10] for current model, Average Loss: 0.0034\n",
      "Starting epoch 4/10 for current model\n",
      "Epoch [4/10] for current model, Average Loss: 0.0026\n",
      "Starting epoch 5/10 for current model\n",
      "Epoch [5/10] for current model, Average Loss: 0.0021\n",
      "Starting epoch 6/10 for current model\n",
      "Epoch [6/10] for current model, Average Loss: 0.0018\n",
      "Starting epoch 7/10 for current model\n",
      "Epoch [7/10] for current model, Average Loss: 0.0015\n",
      "Starting epoch 8/10 for current model\n",
      "Epoch [8/10] for current model, Average Loss: 0.0013\n",
      "Starting epoch 9/10 for current model\n",
      "Epoch [9/10] for current model, Average Loss: 0.0012\n",
      "Starting epoch 10/10 for current model\n",
      "Epoch [10/10] for current model, Average Loss: 0.0010\n",
      "Total number of samples for baseline: 239560\n",
      "Number of batches per epoch for baseline: 234\n",
      "Starting epoch 1/10 for baseline model\n",
      "Epoch [1/10] for baseline model, Average Loss: 0.1075\n",
      "Starting epoch 2/10 for baseline model\n",
      "Epoch [2/10] for baseline model, Average Loss: 0.0049\n",
      "Starting epoch 3/10 for baseline model\n",
      "Epoch [3/10] for baseline model, Average Loss: 0.0032\n",
      "Starting epoch 4/10 for baseline model\n",
      "Epoch [4/10] for baseline model, Average Loss: 0.0029\n",
      "Starting epoch 5/10 for baseline model\n",
      "Epoch [5/10] for baseline model, Average Loss: 0.0025\n",
      "Starting epoch 6/10 for baseline model\n",
      "Epoch [6/10] for baseline model, Average Loss: 0.0021\n",
      "Starting epoch 7/10 for baseline model\n",
      "Epoch [7/10] for baseline model, Average Loss: 0.0017\n",
      "Starting epoch 8/10 for baseline model\n",
      "Epoch [8/10] for baseline model, Average Loss: 0.0013\n",
      "Starting epoch 9/10 for baseline model\n",
      "Epoch [9/10] for baseline model, Average Loss: 0.0012\n",
      "Starting epoch 10/10 for baseline model\n",
      "Epoch [10/10] for baseline model, Average Loss: 0.0011\n",
      "\n",
      "Metrics for current model at horizon: 0.5 seconds (15 frames)\n",
      "RMSE: 0.0258\n",
      "MAE: 0.0162\n",
      "ADE: 0.0259\n",
      "FDE: 0.0278\n",
      "\n",
      "Metrics for current model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0259\n",
      "MAE: 0.0184\n",
      "ADE: 0.0291\n",
      "FDE: 0.0385\n",
      "\n",
      "Metrics for current model at horizon: 1.5 seconds (45 frames)\n",
      "RMSE: 0.0325\n",
      "MAE: 0.0223\n",
      "ADE: 0.0354\n",
      "FDE: 0.0565\n",
      "\n",
      "Metrics for baseline model at horizon: 0.5 seconds (15 frames)\n",
      "RMSE: 0.0264\n",
      "MAE: 0.0154\n",
      "ADE: 0.0252\n",
      "FDE: 0.0239\n",
      "\n",
      "Metrics for baseline model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0256\n",
      "MAE: 0.0164\n",
      "ADE: 0.0268\n",
      "FDE: 0.0354\n",
      "\n",
      "Metrics for baseline model at horizon: 1.5 seconds (45 frames)\n",
      "RMSE: 0.0325\n",
      "MAE: 0.0201\n",
      "ADE: 0.0332\n",
      "FDE: 0.0557\n",
      "Current model saved to eval_model_on_video/output_20241018_080535/trajectory_predictor_current_20241018_080535.pth\n",
      "Current scaler saved to eval_model_on_video/output_20241018_080535/scaler_current_20241018_080535.save\n",
      "Baseline model saved to eval_model_on_video/output_20241018_080535/trajectory_predictor_baseline_20241018_080535.pth\n",
      "Baseline scaler saved to eval_model_on_video/output_20241018_080535/scaler_baseline_20241018_080535.save\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'target_sequences_turn_baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 488\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# Scale targets using baseline scaler\u001b[39;00m\n\u001b[1;32m    486\u001b[0m all_numeric_targets_turn_baseline \u001b[38;5;241m=\u001b[39m target_sequences_turn\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n\u001b[1;32m    487\u001b[0m target_sequences_turn_baseline \u001b[38;5;241m=\u001b[39m scaler_baseline\u001b[38;5;241m.\u001b[39mtransform(all_numeric_targets_turn_baseline)\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m--> 488\u001b[0m     \u001b[43mtarget_sequences_turn_baseline\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], target_sequences_turn_baseline\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[1;32m    491\u001b[0m inputs_turn_baseline \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_sequences_turn_baseline, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_sequences_turn_baseline' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "path = Path('csv_out')\n",
    "eval_video_path = Path('eval_model_on_video')\n",
    "\n",
    "# Create new output directory with timestamp to avoid overwriting\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = eval_video_path / f'output_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Outputs will be saved to: {output_dir}')\n",
    "\n",
    "# Load data\n",
    "df1 = pd.read_csv(path / 'tracking_data.csv')\n",
    "df2 = pd.read_csv(path / 'overall_turn_label.csv')\n",
    "\n",
    "# Merge dataframes\n",
    "df_merged = pd.merge(df1, df2[['id', 'frame', 'overall_turn_label']], on=['id', 'frame'], how='left')\n",
    "\n",
    "# Fill missing 'overall_turn_label' values by forward and backward filling per vehicle id\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='ffill')\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='bfill')\n",
    "\n",
    "# Check for remaining missing values\n",
    "missing_values = df_merged['overall_turn_label'].isnull().sum()\n",
    "print(f\"Number of missing 'overall_turn_label': {missing_values}\")\n",
    "\n",
    "# Fill any remaining missing values with 'straight' (if any)\n",
    "df_merged['overall_turn_label'] = df_merged['overall_turn_label'].fillna('straight')\n",
    "\n",
    "# One-Hot encode 'overall_turn_label'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turn_labels_encoded = encoder.fit_transform(df_merged[['overall_turn_label']])\n",
    "turn_label_columns = encoder.get_feature_names_out(['overall_turn_label'])\n",
    "df_merged[turn_label_columns] = turn_labels_encoded\n",
    "\n",
    "# Define input features\n",
    "input_features = ['center_x', 'center_y'] + list(turn_label_columns)\n",
    "\n",
    "# Define sequence lengths\n",
    "sequence_length = 90  # Input sequence length (90 frames, 3 seconds at 30 fps)\n",
    "predict_length = 45   # Prediction sequence length (45 frames, 1.5 seconds at 30 fps)\n",
    "\n",
    "# Generate input and target sequences for the current model (with turn feature)\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "sequence_vehicle_ids = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  # Only 'center_x' and 'center_y'\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "        sequence_vehicle_ids.append(track_id)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "sequence_vehicle_ids = np.array(sequence_vehicle_ids)\n",
    "\n",
    "# Data normalization\n",
    "numeric_feature_indices = [0, 1]  # Indices for 'center_x' and 'center_y'\n",
    "\n",
    "# Flatten the inputs for scaling\n",
    "all_numeric_inputs = input_sequences[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_numeric_inputs)\n",
    "\n",
    "# Scale inputs\n",
    "input_sequences[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs).reshape(\n",
    "    input_sequences.shape[0], input_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets\n",
    "all_numeric_targets = target_sequences.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences = scaler.transform(all_numeric_targets).reshape(\n",
    "    target_sequences.shape[0], target_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Prepare baseline data (without turn feature)\n",
    "input_features_baseline = ['center_x', 'center_y']\n",
    "\n",
    "input_sequences_baseline = []\n",
    "target_sequences_baseline = []\n",
    "sequence_vehicle_ids_baseline = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features_baseline].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]\n",
    "\n",
    "        input_sequences_baseline.append(input_seq)\n",
    "        target_sequences_baseline.append(target_seq)\n",
    "        sequence_vehicle_ids_baseline.append(track_id)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "input_sequences_baseline = np.array(input_sequences_baseline)\n",
    "target_sequences_baseline = np.array(target_sequences_baseline)\n",
    "sequence_vehicle_ids_baseline = np.array(sequence_vehicle_ids_baseline)\n",
    "\n",
    "# Data normalization for baseline\n",
    "all_numeric_inputs_baseline = input_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler_baseline = StandardScaler()\n",
    "scaler_baseline.fit(all_numeric_inputs_baseline)\n",
    "\n",
    "# Scale inputs\n",
    "input_sequences_baseline = scaler_baseline.transform(all_numeric_inputs_baseline).reshape(\n",
    "    input_sequences_baseline.shape[0], input_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets\n",
    "all_numeric_targets_baseline = target_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_baseline = scaler_baseline.transform(all_numeric_targets_baseline).reshape(\n",
    "    target_sequences_baseline.shape[0], target_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Define the model\n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=2):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        self.lstm_encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_decoder = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, target_len):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Encoder\n",
    "        _, (hidden, cell) = self.lstm_encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_input = x[:, -1, :2].unsqueeze(1)  # Start with the last position of the input sequence\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            out, (hidden, cell) = self.lstm_decoder(decoder_input, (hidden, cell))\n",
    "            out = self.fc_out(out)\n",
    "            outputs.append(out.squeeze(1))\n",
    "            decoder_input = out  # Use current output as next input\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 1024\n",
    "num_epochs = 10\n",
    "target_len = predict_length  # Prediction length\n",
    "\n",
    "# Prepare data for current model\n",
    "input_size = input_sequences.shape[2]  # Number of input features\n",
    "inputs = torch.tensor(input_sequences, dtype=torch.float32).to(device)\n",
    "targets = torch.tensor(target_sequences, dtype=torch.float32).to(device)\n",
    "model = TrajectoryPredictor(input_size=input_size, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "print(f'Number of batches per epoch: {len(data_loader)}')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for current model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for current model')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for current model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# Prepare data for baseline model\n",
    "input_size_baseline = input_sequences_baseline.shape[2]  # Number of input features (without turn feature)\n",
    "inputs_baseline = torch.tensor(input_sequences_baseline, dtype=torch.float32).to(device)\n",
    "targets_baseline = torch.tensor(target_sequences_baseline, dtype=torch.float32).to(device)\n",
    "model_baseline = TrajectoryPredictor(input_size=input_size_baseline, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset_baseline = TensorDataset(inputs_baseline, targets_baseline)\n",
    "data_loader_baseline = DataLoader(dataset_baseline, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples for baseline: {len(dataset_baseline)}')\n",
    "print(f'Number of batches per epoch for baseline: {len(data_loader_baseline)}')\n",
    "\n",
    "# Define loss function and optimizer for baseline model\n",
    "criterion_baseline = nn.MSELoss()\n",
    "optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for baseline model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for baseline model')\n",
    "    model_baseline.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer_baseline.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion_baseline(outputs, batch_targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer_baseline.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader_baseline)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for baseline model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# Define function to compute metrics\n",
    "def compute_metrics(predictions, targets, horizons):\n",
    "    metrics = {}\n",
    "    for horizon in horizons:\n",
    "        outputs_at_horizon = predictions[:, :horizon, :]  # [num_samples, horizon, 2]\n",
    "        targets_at_horizon = targets[:, :horizon, :]\n",
    "\n",
    "        # Compute errors\n",
    "        errors = outputs_at_horizon - targets_at_horizon  # [num_samples, horizon, 2]\n",
    "        squared_errors = errors ** 2\n",
    "        mse = squared_errors.mean().item()\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        abs_errors = errors.abs()\n",
    "        mae = abs_errors.mean().item()\n",
    "\n",
    "        # Compute ADE\n",
    "        displacement_errors = torch.norm(errors, dim=2)  # Euclidean distance over x and y\n",
    "        ade = displacement_errors.mean().item()\n",
    "\n",
    "        # Compute FDE\n",
    "        final_errors = errors[:, -1, :]  # [num_samples, 2]\n",
    "        fde = torch.norm(final_errors, dim=1).mean().item()\n",
    "\n",
    "        metrics[horizon] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'ADE': ade,\n",
    "            'FDE': fde\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Define prediction horizons\n",
    "horizons = {\n",
    "    15: 0.5,  # 0.5 seconds (15 frames)\n",
    "    30: 1.0,  # 1.0 seconds (30 frames)\n",
    "    45: 1.5   # 1.5 seconds (45 frames)\n",
    "}\n",
    "\n",
    "# Filter horizons not exceeding predict_length\n",
    "horizons = {k: v for k, v in horizons.items() if k <= predict_length}\n",
    "\n",
    "# Evaluate current model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs = []\n",
    "    total_targets = []\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "        total_outputs.append(outputs.cpu())\n",
    "        total_targets.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs = torch.cat(total_outputs, dim=0)\n",
    "    total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "    metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "\n",
    "    # Save metrics to file\n",
    "    metrics_file = output_dir / 'metrics_current_model.txt'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f'Metrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "            f.write(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# Evaluate baseline model\n",
    "model_baseline.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs_baseline = []\n",
    "    total_targets_baseline = []\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "        total_outputs_baseline.append(outputs.cpu())\n",
    "        total_targets_baseline.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs_baseline = torch.cat(total_outputs_baseline, dim=0)\n",
    "    total_targets_baseline = torch.cat(total_targets_baseline, dim=0)\n",
    "\n",
    "    metrics_baseline = compute_metrics(total_outputs_baseline, total_targets_baseline, horizons.keys())\n",
    "\n",
    "    # Save metrics to file\n",
    "    metrics_file_baseline = output_dir / 'metrics_baseline_model.txt'\n",
    "    with open(metrics_file_baseline, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f\"Metrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n\")\n",
    "            f.write(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# Save models and scalers\n",
    "model_file = output_dir / f'trajectory_predictor_current_{timestamp}.pth'\n",
    "scaler_file = output_dir / f'scaler_current_{timestamp}.save'\n",
    "\n",
    "torch.save(model.state_dict(), model_file)\n",
    "joblib.dump(scaler, scaler_file)\n",
    "print(f'Current model saved to {model_file}')\n",
    "print(f'Current scaler saved to {scaler_file}')\n",
    "\n",
    "model_file_baseline = output_dir / f'trajectory_predictor_baseline_{timestamp}.pth'\n",
    "scaler_file_baseline = output_dir / f'scaler_baseline_{timestamp}.save'\n",
    "\n",
    "torch.save(model_baseline.state_dict(), model_file_baseline)\n",
    "joblib.dump(scaler_baseline, scaler_file_baseline)\n",
    "print(f'Baseline model saved to {model_file_baseline}')\n",
    "print(f'Baseline scaler saved to {scaler_file_baseline}')\n",
    "\n",
    "# Visualization and metrics comparison code remains the same\n",
    "# ...\n",
    "\n",
    "# ---------------------------------\n",
    "# Evaluate models on turning vehicles\n",
    "# ---------------------------------\n",
    "\n",
    "# 1. Filter data for turning vehicles based on 'overall_turn_label'\n",
    "turn_labels = ['left_turn', 'right_turn']\n",
    "\n",
    "# Since your 'overall_turn_label' represents the true turning status, use it directly\n",
    "turning_df = df_merged[df_merged['overall_turn_label'].isin(turn_labels)]\n",
    "\n",
    "# 2. Prepare sequences for turning vehicles for current model\n",
    "input_sequences_turn = []\n",
    "target_sequences_turn = []\n",
    "\n",
    "grouped_turning = turning_df.groupby('id')\n",
    "\n",
    "for track_id, group in grouped_turning:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  # Only 'center_x' and 'center_y'\n",
    "\n",
    "        input_sequences_turn.append(input_seq)\n",
    "        target_sequences_turn.append(target_seq)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "input_sequences_turn = np.array(input_sequences_turn)\n",
    "target_sequences_turn = np.array(target_sequences_turn)\n",
    "\n",
    "# Data normalization (using the same scaler)\n",
    "numeric_feature_indices = [0, 1]  # 'center_x', 'center_y'\n",
    "\n",
    "# Scale inputs\n",
    "all_numeric_inputs_turn = input_sequences_turn[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs_turn).reshape(\n",
    "    input_sequences_turn.shape[0], input_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets\n",
    "all_numeric_targets_turn = target_sequences_turn.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_turn = scaler.transform(all_numeric_targets_turn).reshape(\n",
    "    target_sequences_turn.shape[0], target_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Create dataset and dataloader\n",
    "inputs_turn = torch.tensor(input_sequences_turn, dtype=torch.float32).to(device)\n",
    "targets_turn = torch.tensor(target_sequences_turn, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset_turn = TensorDataset(inputs_turn, targets_turn)\n",
    "data_loader_turn = DataLoader(dataset_turn, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. Evaluate current model on turning vehicles\n",
    "def evaluate_model_on_turning(model, data_loader, target_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_outputs = []\n",
    "        total_targets = []\n",
    "        for batch_inputs, batch_targets in data_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            outputs = model(batch_inputs, target_len)\n",
    "            total_outputs.append(outputs.cpu())\n",
    "            total_targets.append(batch_targets.cpu())\n",
    "\n",
    "        total_outputs = torch.cat(total_outputs, dim=0)\n",
    "        total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "        metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "    return metrics\n",
    "\n",
    "metrics_turn_current = evaluate_model_on_turning(model, data_loader_turn, target_len)\n",
    "\n",
    "# 4. Prepare data for baseline model on turning vehicles\n",
    "# Remove turn features\n",
    "input_sequences_turn_baseline = input_sequences_turn[:, :, :2]  # Only 'center_x' and 'center_y'\n",
    "\n",
    "# Scale inputs using baseline scaler\n",
    "all_numeric_inputs_turn_baseline = input_sequences_turn_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn_baseline = scaler_baseline.transform(all_numeric_inputs_turn_baseline).reshape(\n",
    "    input_sequences_turn_baseline.shape[0], input_sequences_turn_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets using baseline scaler\n",
    "all_numeric_targets_turn_baseline = target_sequences_turn.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_turn_baseline = scaler_baseline.transform(all_numeric_targets_turn_baseline).reshape(\n",
    "    target_sequences_turn_baseline.shape[0], target_sequences_turn_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Create dataset and dataloader\n",
    "inputs_turn_baseline = torch.tensor(input_sequences_turn_baseline, dtype=torch.float32).to(device)\n",
    "targets_turn_baseline = torch.tensor(target_sequences_turn_baseline, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset_turn_baseline = TensorDataset(inputs_turn_baseline, targets_turn_baseline)\n",
    "data_loader_turn_baseline = DataLoader(dataset_turn_baseline, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate baseline model on turning vehicles\n",
    "metrics_turn_baseline = evaluate_model_on_turning(model_baseline, data_loader_turn_baseline, target_len)\n",
    "\n",
    "# 5. Compare results\n",
    "metrics_file_turn = output_dir / 'metrics_turning_vehicles.txt'\n",
    "with open(metrics_file_turn, 'w') as f:\n",
    "    for horizon_frames, time_sec in horizons.items():\n",
    "        print(f'\\nMetrics on turning vehicles at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "        print(f\"Baseline Model:\")\n",
    "        print(f\"  RMSE: {metrics_turn_baseline[horizon_frames]['RMSE']:.4f}\")\n",
    "        print(f\"  MAE: {metrics_turn_baseline[horizon_frames]['MAE']:.4f}\")\n",
    "        print(f\"  ADE: {metrics_turn_baseline[horizon_frames]['ADE']:.4f}\")\n",
    "        print(f\"  FDE: {metrics_turn_baseline[horizon_frames]['FDE']:.4f}\")\n",
    "        print(f\"Current Model:\")\n",
    "        print(f\"  RMSE: {metrics_turn_current[horizon_frames]['RMSE']:.4f}\")\n",
    "        print(f\"  MAE: {metrics_turn_current[horizon_frames]['MAE']:.4f}\")\n",
    "        print(f\"  ADE: {metrics_turn_current[horizon_frames]['ADE']:.4f}\")\n",
    "        print(f\"  FDE: {metrics_turn_current[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "        f.write(f'Metrics on turning vehicles at horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "        f.write(f\"Baseline Model:\\n\")\n",
    "        f.write(f\"  RMSE: {metrics_turn_baseline[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "        f.write(f\"  MAE: {metrics_turn_baseline[horizon_frames]['MAE']:.4f}\\n\")\n",
    "        f.write(f\"  ADE: {metrics_turn_baseline[horizon_frames]['ADE']:.4f}\\n\")\n",
    "        f.write(f\"  FDE: {metrics_turn_baseline[horizon_frames]['FDE']:.4f}\\n\")\n",
    "        f.write(f\"Current Model:\\n\")\n",
    "        f.write(f\"  RMSE: {metrics_turn_current[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "        f.write(f\"  MAE: {metrics_turn_current[horizon_frames]['MAE']:.4f}\\n\")\n",
    "        f.write(f\"  ADE: {metrics_turn_current[horizon_frames]['ADE']:.4f}\\n\")\n",
    "        f.write(f\"  FDE: {metrics_turn_current[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "print(f\"Metrics on turning vehicles saved to {metrics_file_turn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e3d0b2-4a3b-42da-bccc-fbf7ce4b2edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: eval_model_on_video/output_20241018_082033\n",
      "Number of missing 'overall_turn_label': 0\n",
      "Using device: cuda\n",
      "Total number of samples: 239560\n",
      "Number of batches per epoch: 234\n",
      "Starting epoch 1/10 for current model\n",
      "Epoch [1/10] for current model, Average Loss: 0.1221\n",
      "Starting epoch 2/10 for current model\n",
      "Epoch [2/10] for current model, Average Loss: 0.0044\n",
      "Starting epoch 3/10 for current model\n",
      "Epoch [3/10] for current model, Average Loss: 0.0032\n",
      "Starting epoch 4/10 for current model\n",
      "Epoch [4/10] for current model, Average Loss: 0.0026\n",
      "Starting epoch 5/10 for current model\n",
      "Epoch [5/10] for current model, Average Loss: 0.0022\n",
      "Starting epoch 6/10 for current model\n",
      "Epoch [6/10] for current model, Average Loss: 0.0016\n",
      "Starting epoch 7/10 for current model\n",
      "Epoch [7/10] for current model, Average Loss: 0.0403\n",
      "Starting epoch 8/10 for current model\n",
      "Epoch [8/10] for current model, Average Loss: 0.0051\n",
      "Starting epoch 9/10 for current model\n",
      "Epoch [9/10] for current model, Average Loss: 0.0014\n",
      "Starting epoch 10/10 for current model\n",
      "Epoch [10/10] for current model, Average Loss: 0.0011\n",
      "Total number of samples for baseline: 239560\n",
      "Number of batches per epoch for baseline: 234\n",
      "Starting epoch 1/10 for baseline model\n",
      "Epoch [1/10] for baseline model, Average Loss: 0.1143\n",
      "Starting epoch 2/10 for baseline model\n",
      "Epoch [2/10] for baseline model, Average Loss: 0.0053\n",
      "Starting epoch 3/10 for baseline model\n",
      "Epoch [3/10] for baseline model, Average Loss: 0.0035\n",
      "Starting epoch 4/10 for baseline model\n",
      "Epoch [4/10] for baseline model, Average Loss: 0.0029\n",
      "Starting epoch 5/10 for baseline model\n",
      "Epoch [5/10] for baseline model, Average Loss: 0.0022\n",
      "Starting epoch 6/10 for baseline model\n",
      "Epoch [6/10] for baseline model, Average Loss: 0.0020\n",
      "Starting epoch 7/10 for baseline model\n",
      "Epoch [7/10] for baseline model, Average Loss: 0.0015\n",
      "Starting epoch 8/10 for baseline model\n",
      "Epoch [8/10] for baseline model, Average Loss: 0.0014\n",
      "Starting epoch 9/10 for baseline model\n",
      "Epoch [9/10] for baseline model, Average Loss: 0.0012\n",
      "Starting epoch 10/10 for baseline model\n",
      "Epoch [10/10] for baseline model, Average Loss: 0.0011\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 412\u001b[0m\n\u001b[1;32m    408\u001b[0m predictions_turn_baseline_unscaled \u001b[38;5;241m=\u001b[39m scaler_baseline\u001b[38;5;241m.\u001b[39minverse_transform(predictions_turn_baseline\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    409\u001b[0m     predictions_turn_baseline\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], predictions_turn_baseline\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# 反标准化目标\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m targets_turn_unscaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets_turn_current\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    413\u001b[0m     targets_turn_current\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], targets_turn_current\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# 历史输入（用于可视化）\u001b[39;00m\n\u001b[1;32m    416\u001b[0m history_inputs_unscaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(\n\u001b[1;32m    417\u001b[0m     inputs_turn_current[:, :, numeric_feature_indices]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n\u001b[1;32m    418\u001b[0m )\u001b[38;5;241m.\u001b[39mreshape(inputs_turn_current\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], inputs_turn_current\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n",
      "File \u001b[0;32m~/anaconda3/envs/drone_detection/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1088\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1085\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1087\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1088\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/anaconda3/envs/drone_detection/lib/python3.10/site-packages/sklearn/utils/validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/drone_detection/lib/python3.10/site-packages/sklearn/utils/_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[0;32m~/anaconda3/envs/drone_detection/lib/python3.10/site-packages/torch/_tensor.py:1085\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 定义路径\n",
    "path = Path('csv_out')\n",
    "eval_video_path = Path('eval_model_on_video')\n",
    "\n",
    "# 创建带有时间戳的新输出目录，避免覆盖\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = eval_video_path / f'output_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Outputs will be saved to: {output_dir}')\n",
    "\n",
    "# 加载数据\n",
    "df1 = pd.read_csv(path / 'tracking_data.csv')\n",
    "df2 = pd.read_csv(path / 'overall_turn_label.csv')\n",
    "\n",
    "# 合并数据帧\n",
    "df_merged = pd.merge(df1, df2[['id', 'frame', 'overall_turn_label']], on=['id', 'frame'], how='left')\n",
    "\n",
    "# 按车辆ID填充缺失的 'overall_turn_label' 值（前向和后向填充）\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='ffill')\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='bfill')\n",
    "\n",
    "# 检查剩余的缺失值\n",
    "missing_values = df_merged['overall_turn_label'].isnull().sum()\n",
    "print(f\"Number of missing 'overall_turn_label': {missing_values}\")\n",
    "\n",
    "# 填充任何剩余的缺失值为 'straight'（如果有）\n",
    "df_merged['overall_turn_label'] = df_merged['overall_turn_label'].fillna('straight')\n",
    "\n",
    "# One-Hot 编码 'overall_turn_label'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turn_labels_encoded = encoder.fit_transform(df_merged[['overall_turn_label']])\n",
    "turn_label_columns = encoder.get_feature_names_out(['overall_turn_label'])\n",
    "df_merged[turn_label_columns] = turn_labels_encoded\n",
    "\n",
    "# 定义输入特征\n",
    "input_features = ['center_x', 'center_y'] + list(turn_label_columns)\n",
    "\n",
    "# 定义序列长度\n",
    "sequence_length = 90  # 输入序列长度（90帧，30fps下为3秒）\n",
    "predict_length = 45   # 预测序列长度（45帧，30fps下为1.5秒）\n",
    "\n",
    "# 生成当前模型的输入和目标序列（包含转弯特征）\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "sequence_vehicle_ids = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  # 只取 'center_x' 和 'center_y'\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "        sequence_vehicle_ids.append(track_id)\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "sequence_vehicle_ids = np.array(sequence_vehicle_ids)\n",
    "\n",
    "# 数据标准化\n",
    "numeric_feature_indices = [0, 1]  # 'center_x' 和 'center_y' 的索引\n",
    "\n",
    "# 扁平化输入以进行缩放\n",
    "all_numeric_inputs = input_sequences[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_numeric_inputs)\n",
    "\n",
    "# 缩放输入\n",
    "input_sequences[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs).reshape(\n",
    "    input_sequences.shape[0], input_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 缩放目标\n",
    "all_numeric_targets = target_sequences.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences = scaler.transform(all_numeric_targets).reshape(\n",
    "    target_sequences.shape[0], target_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 准备基线模型的数据（不包含转弯特征）\n",
    "input_features_baseline = ['center_x', 'center_y']\n",
    "\n",
    "input_sequences_baseline = []\n",
    "target_sequences_baseline = []\n",
    "sequence_vehicle_ids_baseline = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features_baseline].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]\n",
    "\n",
    "        input_sequences_baseline.append(input_seq)\n",
    "        target_sequences_baseline.append(target_seq)\n",
    "        sequence_vehicle_ids_baseline.append(track_id)\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "input_sequences_baseline = np.array(input_sequences_baseline)\n",
    "target_sequences_baseline = np.array(target_sequences_baseline)\n",
    "sequence_vehicle_ids_baseline = np.array(sequence_vehicle_ids_baseline)\n",
    "\n",
    "# 基线模型的数据标准化\n",
    "all_numeric_inputs_baseline = input_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler_baseline = StandardScaler()\n",
    "scaler_baseline.fit(all_numeric_inputs_baseline)\n",
    "\n",
    "# 缩放输入\n",
    "input_sequences_baseline = scaler_baseline.transform(all_numeric_inputs_baseline).reshape(\n",
    "    input_sequences_baseline.shape[0], input_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 缩放目标\n",
    "all_numeric_targets_baseline = target_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_baseline = scaler_baseline.transform(all_numeric_targets_baseline).reshape(\n",
    "    target_sequences_baseline.shape[0], target_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 定义模型\n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=2):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        self.lstm_encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_decoder = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, target_len):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 编码器\n",
    "        _, (hidden, cell) = self.lstm_encoder(x)\n",
    "\n",
    "        # 解码器\n",
    "        decoder_input = x[:, -1, :2].unsqueeze(1)  # 从输入序列的最后一个位置开始\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            out, (hidden, cell) = self.lstm_decoder(decoder_input, (hidden, cell))\n",
    "            out = self.fc_out(out)\n",
    "            outputs.append(out.squeeze(1))\n",
    "            decoder_input = out  # 使用当前输出作为下一个输入\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 训练参数\n",
    "batch_size = 1024\n",
    "num_epochs = 10\n",
    "target_len = predict_length  # 预测长度\n",
    "\n",
    "# 准备当前模型的数据\n",
    "input_size = input_sequences.shape[2]  # 输入特征数量\n",
    "inputs = torch.tensor(input_sequences, dtype=torch.float32).to(device)\n",
    "targets = torch.tensor(target_sequences, dtype=torch.float32).to(device)\n",
    "model = TrajectoryPredictor(input_size=input_size, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "print(f'Number of batches per epoch: {len(data_loader)}')\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 当前模型的训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for current model')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for current model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 准备基线模型的数据\n",
    "input_size_baseline = input_sequences_baseline.shape[2]  # 输入特征数量（不包含转弯特征）\n",
    "inputs_baseline = torch.tensor(input_sequences_baseline, dtype=torch.float32).to(device)\n",
    "targets_baseline = torch.tensor(target_sequences_baseline, dtype=torch.float32).to(device)\n",
    "model_baseline = TrajectoryPredictor(input_size=input_size_baseline, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset_baseline = TensorDataset(inputs_baseline, targets_baseline)\n",
    "data_loader_baseline = DataLoader(dataset_baseline, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples for baseline: {len(dataset_baseline)}')\n",
    "print(f'Number of batches per epoch for baseline: {len(data_loader_baseline)}')\n",
    "\n",
    "# 定义基线模型的损失函数和优化器\n",
    "criterion_baseline = nn.MSELoss()\n",
    "optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=0.001)\n",
    "\n",
    "# 基线模型的训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for baseline model')\n",
    "    model_baseline.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer_baseline.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion_baseline(outputs, batch_targets)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer_baseline.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader_baseline)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for baseline model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 定义函数来计算指标\n",
    "def compute_metrics(predictions, targets, horizons):\n",
    "    metrics = {}\n",
    "    for horizon in horizons:\n",
    "        outputs_at_horizon = predictions[:, :horizon, :]  # [num_samples, horizon, 2]\n",
    "        targets_at_horizon = targets[:, :horizon, :]\n",
    "\n",
    "        # 计算误差\n",
    "        errors = outputs_at_horizon - targets_at_horizon  # [num_samples, horizon, 2]\n",
    "        squared_errors = errors ** 2\n",
    "        mse = squared_errors.mean().item()\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        abs_errors = errors.abs()\n",
    "        mae = abs_errors.mean().item()\n",
    "\n",
    "        # 计算 ADE\n",
    "        displacement_errors = torch.norm(errors, dim=2)  # 计算 x 和 y 上的欧氏距离\n",
    "        ade = displacement_errors.mean().item()\n",
    "\n",
    "        # 计算 FDE\n",
    "        final_errors = errors[:, -1, :]  # [num_samples, 2]\n",
    "        fde = torch.norm(final_errors, dim=1).mean().item()\n",
    "\n",
    "        metrics[horizon] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'ADE': ade,\n",
    "            'FDE': fde\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# 定义预测地平线\n",
    "horizons = {\n",
    "    15: 0.5,  # 0.5 秒（15 帧）\n",
    "    30: 1.0,  # 1.0 秒（30 帧）\n",
    "    45: 1.5   # 1.5 秒（45 帧）\n",
    "}\n",
    "\n",
    "# 过滤不超过 predict_length 的地平线\n",
    "horizons = {k: v for k, v in horizons.items() if k <= predict_length}\n",
    "\n",
    "# -----------------------------\n",
    "# 模型在转弯车辆上的预测和比较\n",
    "# -----------------------------\n",
    "\n",
    "# 1. 筛选转弯车辆的序列\n",
    "turn_labels = ['left_turn', 'right_turn']\n",
    "turning_df = df_merged[df_merged['overall_turn_label'].isin(turn_labels)]\n",
    "\n",
    "# 2. 准备转弯车辆的输入和目标序列\n",
    "input_sequences_turn = []\n",
    "target_sequences_turn = []\n",
    "sequence_indices_turn = []\n",
    "\n",
    "grouped_turning = turning_df.groupby('id')\n",
    "\n",
    "for track_id, group in grouped_turning:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features_current = group[input_features].values  # 包含转弯特征\n",
    "    features_baseline = group[['center_x', 'center_y']].values  # 不包含转弯特征\n",
    "\n",
    "    num_sequences = len(features_current) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq_current = features_current[i:i + sequence_length]\n",
    "        input_seq_baseline = features_baseline[i:i + sequence_length]\n",
    "        target_seq = features_current[i + sequence_length:i + sequence_length + predict_length, :2]  # 只取 'center_x' 和 'center_y'\n",
    "\n",
    "        input_sequences_turn.append((input_seq_current, input_seq_baseline))\n",
    "        target_sequences_turn.append(target_seq)\n",
    "        sequence_indices_turn.append((track_id, i))\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "input_sequences_turn_current = np.array([seq[0] for seq in input_sequences_turn])\n",
    "input_sequences_turn_baseline = np.array([seq[1] for seq in input_sequences_turn])\n",
    "target_sequences_turn = np.array(target_sequences_turn)\n",
    "\n",
    "# 对当前模型的数据进行标准化\n",
    "# 输入\n",
    "all_numeric_inputs_turn_current = input_sequences_turn_current[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn_current[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs_turn_current).reshape(\n",
    "    input_sequences_turn_current.shape[0], input_sequences_turn_current.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 目标\n",
    "all_numeric_targets_turn = target_sequences_turn.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_turn_scaled = scaler.transform(all_numeric_targets_turn).reshape(\n",
    "    target_sequences_turn.shape[0], target_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 对基线模型的数据进行标准化\n",
    "# 输入\n",
    "all_numeric_inputs_turn_baseline = input_sequences_turn_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn_baseline = scaler_baseline.transform(all_numeric_inputs_turn_baseline).reshape(\n",
    "    input_sequences_turn_baseline.shape[0], input_sequences_turn_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 目标\n",
    "# 目标序列对于两个模型是相同的，但要使用各自的 scaler 进行标准化\n",
    "target_sequences_turn_scaled_baseline = scaler_baseline.transform(all_numeric_targets_turn).reshape(\n",
    "    target_sequences_turn.shape[0], target_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 转换为张量\n",
    "inputs_turn_current = torch.tensor(input_sequences_turn_current, dtype=torch.float32).to(device)\n",
    "inputs_turn_baseline = torch.tensor(input_sequences_turn_baseline, dtype=torch.float32).to(device)\n",
    "targets_turn_current = torch.tensor(target_sequences_turn_scaled, dtype=torch.float32).to(device)\n",
    "targets_turn_baseline = torch.tensor(target_sequences_turn_scaled_baseline, dtype=torch.float32).to(device)\n",
    "\n",
    "# 定义数据集和数据加载器\n",
    "dataset_turn_current = TensorDataset(inputs_turn_current, targets_turn_current)\n",
    "dataset_turn_baseline = TensorDataset(inputs_turn_baseline, targets_turn_baseline)\n",
    "\n",
    "data_loader_turn_current = DataLoader(dataset_turn_current, batch_size=batch_size, shuffle=False)\n",
    "data_loader_turn_baseline = DataLoader(dataset_turn_baseline, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 获取模型预测\n",
    "def get_predictions(model, data_loader, target_len):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, _ in data_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            outputs = model(batch_inputs, target_len)\n",
    "            predictions.append(outputs.cpu())\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    return predictions\n",
    "\n",
    "# 获取当前模型的预测\n",
    "predictions_turn_current = get_predictions(model, data_loader_turn_current, target_len)\n",
    "\n",
    "# 获取基线模型的预测\n",
    "predictions_turn_baseline = get_predictions(model_baseline, data_loader_turn_baseline, target_len)\n",
    "\n",
    "# 反标准化预测结果\n",
    "# 反标准化当前模型的预测\n",
    "predictions_turn_current_unscaled = scaler.inverse_transform(predictions_turn_current.reshape(-1, 2)).reshape(\n",
    "    predictions_turn_current.shape[0], predictions_turn_current.shape[1], 2)\n",
    "\n",
    "# 反标准化基线模型的预测\n",
    "predictions_turn_baseline_unscaled = scaler_baseline.inverse_transform(predictions_turn_baseline.reshape(-1, 2)).reshape(\n",
    "    predictions_turn_baseline.shape[0], predictions_turn_baseline.shape[1], 2)\n",
    "\n",
    "# 反标准化目标\n",
    "targets_turn_unscaled = scaler.inverse_transform(targets_turn_current.reshape(-1, 2)).reshape(\n",
    "    targets_turn_current.shape[0], targets_turn_current.shape[1], 2)\n",
    "\n",
    "# 历史输入（用于可视化）\n",
    "history_inputs_unscaled = scaler.inverse_transform(\n",
    "    inputs_turn_current[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    ").reshape(inputs_turn_current.shape[0], inputs_turn_current.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 定义函数来计算轨迹的曲率\n",
    "def compute_trajectory_curvature(traj):\n",
    "    # traj: [seq_len, 2]\n",
    "    dx = np.gradient(traj[:, 0])\n",
    "    dy = np.gradient(traj[:, 1])\n",
    "    ddx = np.gradient(dx)\n",
    "    ddy = np.gradient(dy)\n",
    "    curvature = np.abs(dx * ddy - dy * ddx) / (dx * dx + dy * dy) ** 1.5\n",
    "    curvature[np.isnan(curvature)] = 0\n",
    "    curvature[np.isinf(curvature)] = 0\n",
    "    return curvature\n",
    "\n",
    "# 选择案例\n",
    "num_samples = 5  # 可视化的样本数量\n",
    "selected_indices = []\n",
    "\n",
    "for idx in range(predictions_turn_current_unscaled.shape[0]):\n",
    "    # 计算基线模型和当前模型预测的曲率\n",
    "    curvature_baseline = compute_trajectory_curvature(predictions_turn_baseline_unscaled[idx])\n",
    "    curvature_current = compute_trajectory_curvature(predictions_turn_current_unscaled[idx])\n",
    "\n",
    "    # 计算曲率的平均值\n",
    "    avg_curvature_baseline = np.mean(curvature_baseline)\n",
    "    avg_curvature_current = np.mean(curvature_current)\n",
    "\n",
    "    # 如果基线模型的曲率较小（预测为直行），而当前模型的曲率较大（预测为转弯），则选择该样本\n",
    "    if avg_curvature_baseline < 0.001 and avg_curvature_current > avg_curvature_baseline + 0.005:\n",
    "        selected_indices.append(idx)\n",
    "\n",
    "    if len(selected_indices) >= num_samples:\n",
    "        break\n",
    "\n",
    "# 可视化选定的案例\n",
    "for idx in selected_indices:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # 绘制历史轨迹\n",
    "    plt.plot(history_inputs_unscaled[idx, :, 0], history_inputs_unscaled[idx, :, 1], 'bo-', label='历史轨迹')\n",
    "\n",
    "    # 绘制真实未来轨迹\n",
    "    plt.plot(targets_turn_unscaled[idx, :, 0], targets_turn_unscaled[idx, :, 1], 'go-', label='真实未来轨迹')\n",
    "\n",
    "    # 绘制基线模型预测\n",
    "    plt.plot(predictions_turn_baseline_unscaled[idx, :, 0], predictions_turn_baseline_unscaled[idx, :, 1], 'ro--', label='基线模型预测')\n",
    "\n",
    "    # 绘制当前模型预测\n",
    "    plt.plot(predictions_turn_current_unscaled[idx, :, 0], predictions_turn_current_unscaled[idx, :, 1], 'co--', label='当前模型预测')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('center_x')\n",
    "    plt.ylabel('center_y')\n",
    "    vehicle_id, seq_idx = sequence_indices_turn[idx]\n",
    "    plt.title(f'车辆ID {vehicle_id} 序列 {seq_idx} 的轨迹预测比较')\n",
    "\n",
    "    # 保存或显示图像\n",
    "    # plt.show()\n",
    "    # 或者保存图像\n",
    "    figure_path = output_dir / f'Comparison_vehicle_{vehicle_id}_seq_{seq_idx}_{timestamp}.png'\n",
    "    plt.savefig(figure_path)\n",
    "    plt.close()\n",
    "    print(f'Plot saved to {figure_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4a2bc7-f8d0-491c-8155-28bcab9bbc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: eval_model_on_video/output_20241018_082709\n",
      "Number of missing 'overall_turn_label': 0\n",
      "Using device: cuda\n",
      "Total number of samples: 212539\n",
      "Number of batches per epoch: 416\n",
      "Starting epoch 1/10 for current model\n",
      "Epoch [1/10] for current model, Average Loss: 0.1269\n",
      "Starting epoch 2/10 for current model\n",
      "Epoch [2/10] for current model, Average Loss: 0.0067\n",
      "Starting epoch 3/10 for current model\n",
      "Epoch [3/10] for current model, Average Loss: 0.0046\n",
      "Starting epoch 4/10 for current model\n",
      "Epoch [4/10] for current model, Average Loss: 0.0043\n",
      "Starting epoch 5/10 for current model\n",
      "Epoch [5/10] for current model, Average Loss: 0.0218\n",
      "Starting epoch 6/10 for current model\n",
      "Epoch [6/10] for current model, Average Loss: 0.0047\n",
      "Starting epoch 7/10 for current model\n",
      "Epoch [7/10] for current model, Average Loss: 0.0033\n",
      "Starting epoch 8/10 for current model\n",
      "Epoch [8/10] for current model, Average Loss: 0.0028\n",
      "Starting epoch 9/10 for current model\n",
      "Epoch [9/10] for current model, Average Loss: 0.0033\n",
      "Starting epoch 10/10 for current model\n",
      "Epoch [10/10] for current model, Average Loss: 0.0026\n",
      "Total number of samples for baseline: 212539\n",
      "Number of batches per epoch for baseline: 416\n",
      "Starting epoch 1/10 for baseline model\n",
      "Epoch [1/10] for baseline model, Average Loss: 0.4205\n",
      "Starting epoch 2/10 for baseline model\n",
      "Epoch [2/10] for baseline model, Average Loss: 0.1072\n",
      "Starting epoch 3/10 for baseline model\n",
      "Epoch [3/10] for baseline model, Average Loss: 0.0280\n",
      "Starting epoch 4/10 for baseline model\n",
      "Epoch [4/10] for baseline model, Average Loss: 0.0098\n",
      "Starting epoch 5/10 for baseline model\n",
      "Epoch [5/10] for baseline model, Average Loss: 0.0065\n",
      "Starting epoch 6/10 for baseline model\n",
      "Epoch [6/10] for baseline model, Average Loss: 0.0049\n",
      "Starting epoch 7/10 for baseline model\n",
      "Epoch [7/10] for baseline model, Average Loss: 0.0043\n",
      "Starting epoch 8/10 for baseline model\n",
      "Epoch [8/10] for baseline model, Average Loss: 0.0051\n",
      "Starting epoch 9/10 for baseline model\n",
      "Epoch [9/10] for baseline model, Average Loss: 0.0082\n",
      "Starting epoch 10/10 for baseline model\n",
      "Epoch [10/10] for baseline model, Average Loss: 0.0062\n",
      "\n",
      "Metrics for current model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0285\n",
      "MAE: 0.0171\n",
      "ADE: 0.0274\n",
      "FDE: 0.0312\n",
      "\n",
      "Metrics for current model at horizon: 2.0 seconds (60 frames)\n",
      "RMSE: 0.0366\n",
      "MAE: 0.0208\n",
      "ADE: 0.0333\n",
      "FDE: 0.0486\n",
      "\n",
      "Metrics for current model at horizon: 3.0 seconds (90 frames)\n",
      "RMSE: 0.0521\n",
      "MAE: 0.0268\n",
      "ADE: 0.0426\n",
      "FDE: 0.0738\n",
      "\n",
      "Metrics for baseline model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0336\n",
      "MAE: 0.0221\n",
      "ADE: 0.0352\n",
      "FDE: 0.0366\n",
      "\n",
      "Metrics for baseline model at horizon: 2.0 seconds (60 frames)\n",
      "RMSE: 0.0441\n",
      "MAE: 0.0241\n",
      "ADE: 0.0387\n",
      "FDE: 0.0478\n",
      "\n",
      "Metrics for baseline model at horizon: 3.0 seconds (90 frames)\n",
      "RMSE: 0.0605\n",
      "MAE: 0.0275\n",
      "ADE: 0.0444\n",
      "FDE: 0.0647\n",
      "Current model saved to eval_model_on_video/output_20241018_082709/trajectory_predictor_current_20241018_082709.pth\n",
      "Current scaler saved to eval_model_on_video/output_20241018_082709/scaler_current_20241018_082709.save\n",
      "Baseline model saved to eval_model_on_video/output_20241018_082709/trajectory_predictor_baseline_20241018_082709.pth\n",
      "Baseline scaler saved to eval_model_on_video/output_20241018_082709/scaler_baseline_20241018_082709.save\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'target_sequences_turn_baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 483\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# 使用基线模型的 scaler 缩放目标\u001b[39;00m\n\u001b[1;32m    481\u001b[0m all_numeric_targets_turn_baseline \u001b[38;5;241m=\u001b[39m target_sequences_turn\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n\u001b[1;32m    482\u001b[0m target_sequences_turn_baseline \u001b[38;5;241m=\u001b[39m scaler_baseline\u001b[38;5;241m.\u001b[39mtransform(all_numeric_targets_turn_baseline)\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m--> 483\u001b[0m     \u001b[43mtarget_sequences_turn_baseline\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], target_sequences_turn_baseline\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# 创建数据集和数据加载器\u001b[39;00m\n\u001b[1;32m    486\u001b[0m inputs_turn_baseline \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_sequences_turn_baseline, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_sequences_turn_baseline' is not defined"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 定义路径\n",
    "path = Path('csv_out')\n",
    "eval_video_path = Path('eval_model_on_video')\n",
    "\n",
    "# 创建带有时间戳的新输出目录，避免覆盖\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = eval_video_path / f'output_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Outputs will be saved to: {output_dir}')\n",
    "\n",
    "# 加载数据\n",
    "df1 = pd.read_csv(path / 'tracking_data.csv')\n",
    "df2 = pd.read_csv(path / 'overall_turn_label.csv')\n",
    "\n",
    "# 合并数据帧\n",
    "df_merged = pd.merge(df1, df2[['id', 'frame', 'overall_turn_label']], on=['id', 'frame'], how='left')\n",
    "\n",
    "# 按车辆ID填充缺失的 'overall_turn_label' 值（前向和后向填充）\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='ffill')\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='bfill')\n",
    "\n",
    "# 检查剩余的缺失值\n",
    "missing_values = df_merged['overall_turn_label'].isnull().sum()\n",
    "print(f\"Number of missing 'overall_turn_label': {missing_values}\")\n",
    "\n",
    "# 填充任何剩余的缺失值为 'straight'（如果有）\n",
    "df_merged['overall_turn_label'] = df_merged['overall_turn_label'].fillna('straight')\n",
    "\n",
    "# One-Hot 编码 'overall_turn_label'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turn_labels_encoded = encoder.fit_transform(df_merged[['overall_turn_label']])\n",
    "turn_label_columns = encoder.get_feature_names_out(['overall_turn_label'])\n",
    "df_merged[turn_label_columns] = turn_labels_encoded\n",
    "\n",
    "# 定义输入特征\n",
    "input_features = ['center_x', 'center_y'] + list(turn_label_columns)\n",
    "\n",
    "# 定义序列长度\n",
    "sequence_length = 180  # 输入序列长度（180帧，30fps下为6秒）\n",
    "predict_length = 90    # 最大预测序列长度（90帧，30fps下为3秒）\n",
    "\n",
    "# 定义预测地平线\n",
    "horizons = {\n",
    "    30: 1.0,  # 1.0 秒（30 帧）\n",
    "    60: 2.0,  # 2.0 秒（60 帧）\n",
    "    90: 3.0   # 3.0 秒（90 帧）\n",
    "}\n",
    "\n",
    "# 生成当前模型的输入和目标序列（包含转弯特征）\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "sequence_vehicle_ids = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  # 只取 'center_x' 和 'center_y'\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "        sequence_vehicle_ids.append(track_id)\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "sequence_vehicle_ids = np.array(sequence_vehicle_ids)\n",
    "\n",
    "# 数据标准化\n",
    "numeric_feature_indices = [0, 1]  # 'center_x' 和 'center_y' 的索引\n",
    "\n",
    "# 扁平化输入以进行缩放\n",
    "all_numeric_inputs = input_sequences[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_numeric_inputs)\n",
    "\n",
    "# 缩放输入\n",
    "input_sequences[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs).reshape(\n",
    "    input_sequences.shape[0], input_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 缩放目标\n",
    "all_numeric_targets = target_sequences.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences = scaler.transform(all_numeric_targets).reshape(\n",
    "    target_sequences.shape[0], target_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 准备基线模型的数据（不包含转弯特征）\n",
    "input_features_baseline = ['center_x', 'center_y']\n",
    "\n",
    "input_sequences_baseline = []\n",
    "target_sequences_baseline = []\n",
    "sequence_vehicle_ids_baseline = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features_baseline].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]\n",
    "\n",
    "        input_sequences_baseline.append(input_seq)\n",
    "        target_sequences_baseline.append(target_seq)\n",
    "        sequence_vehicle_ids_baseline.append(track_id)\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "input_sequences_baseline = np.array(input_sequences_baseline)\n",
    "target_sequences_baseline = np.array(target_sequences_baseline)\n",
    "sequence_vehicle_ids_baseline = np.array(sequence_vehicle_ids_baseline)\n",
    "\n",
    "# 基线模型的数据标准化\n",
    "all_numeric_inputs_baseline = input_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler_baseline = StandardScaler()\n",
    "scaler_baseline.fit(all_numeric_inputs_baseline)\n",
    "\n",
    "# 缩放输入\n",
    "input_sequences_baseline = scaler_baseline.transform(all_numeric_inputs_baseline).reshape(\n",
    "    input_sequences_baseline.shape[0], input_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 缩放目标\n",
    "all_numeric_targets_baseline = target_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_baseline = scaler_baseline.transform(all_numeric_targets_baseline).reshape(\n",
    "    target_sequences_baseline.shape[0], target_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 定义模型\n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=2):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        self.lstm_encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_decoder = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, target_len):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 编码器\n",
    "        _, (hidden, cell) = self.lstm_encoder(x)\n",
    "\n",
    "        # 解码器\n",
    "        decoder_input = x[:, -1, :2].unsqueeze(1)  # 从输入序列的最后一个位置开始\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            out, (hidden, cell) = self.lstm_decoder(decoder_input, (hidden, cell))\n",
    "            out = self.fc_out(out)\n",
    "            outputs.append(out.squeeze(1))\n",
    "            decoder_input = out  # 使用当前输出作为下一个输入\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 训练参数\n",
    "batch_size = 512  # 根据内存情况调整批次大小\n",
    "num_epochs = 10\n",
    "target_len = predict_length  # 最大预测长度\n",
    "\n",
    "# 准备当前模型的数据\n",
    "input_size = input_sequences.shape[2]  # 输入特征数量\n",
    "inputs = torch.tensor(input_sequences, dtype=torch.float32).to(device)\n",
    "targets = torch.tensor(target_sequences, dtype=torch.float32).to(device)\n",
    "model = TrajectoryPredictor(input_size=input_size, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "print(f'Number of batches per epoch: {len(data_loader)}')\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 当前模型的训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for current model')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for current model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 准备基线模型的数据\n",
    "input_size_baseline = input_sequences_baseline.shape[2]  # 输入特征数量（不包含转弯特征）\n",
    "inputs_baseline = torch.tensor(input_sequences_baseline, dtype=torch.float32).to(device)\n",
    "targets_baseline = torch.tensor(target_sequences_baseline, dtype=torch.float32).to(device)\n",
    "model_baseline = TrajectoryPredictor(input_size=input_size_baseline, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset_baseline = TensorDataset(inputs_baseline, targets_baseline)\n",
    "data_loader_baseline = DataLoader(dataset_baseline, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples for baseline: {len(dataset_baseline)}')\n",
    "print(f'Number of batches per epoch for baseline: {len(data_loader_baseline)}')\n",
    "\n",
    "# 定义基线模型的损失函数和优化器\n",
    "criterion_baseline = nn.MSELoss()\n",
    "optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=0.001)\n",
    "\n",
    "# 基线模型的训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for baseline model')\n",
    "    model_baseline.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer_baseline.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion_baseline(outputs, batch_targets)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer_baseline.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader_baseline)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for baseline model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 定义函数来计算指标\n",
    "def compute_metrics(predictions, targets, horizons):\n",
    "    metrics = {}\n",
    "    for horizon in horizons:\n",
    "        outputs_at_horizon = predictions[:, :horizon, :]  # [num_samples, horizon, 2]\n",
    "        targets_at_horizon = targets[:, :horizon, :]\n",
    "\n",
    "        # 计算误差\n",
    "        errors = outputs_at_horizon - targets_at_horizon  # [num_samples, horizon, 2]\n",
    "        squared_errors = errors ** 2\n",
    "        mse = squared_errors.mean().item()\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        abs_errors = errors.abs()\n",
    "        mae = abs_errors.mean().item()\n",
    "\n",
    "        # 计算 ADE\n",
    "        displacement_errors = torch.norm(errors, dim=2)  # 计算 x 和 y 上的欧氏距离\n",
    "        ade = displacement_errors.mean().item()\n",
    "\n",
    "        # 计算 FDE\n",
    "        final_errors = errors[:, -1, :]  # [num_samples, 2]\n",
    "        fde = torch.norm(final_errors, dim=1).mean().item()\n",
    "\n",
    "        metrics[horizon] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'ADE': ade,\n",
    "            'FDE': fde\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# 评估当前模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs = []\n",
    "    total_targets = []\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "        total_outputs.append(outputs.cpu())\n",
    "        total_targets.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs = torch.cat(total_outputs, dim=0)\n",
    "    total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "    metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "\n",
    "    # 保存指标到文件\n",
    "    metrics_file = output_dir / 'metrics_current_model.txt'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f'Metrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "            f.write(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# 评估基线模型\n",
    "model_baseline.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs_baseline = []\n",
    "    total_targets_baseline = []\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "        total_outputs_baseline.append(outputs.cpu())\n",
    "        total_targets_baseline.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs_baseline = torch.cat(total_outputs_baseline, dim=0)\n",
    "    total_targets_baseline = torch.cat(total_targets_baseline, dim=0)\n",
    "\n",
    "    metrics_baseline = compute_metrics(total_outputs_baseline, total_targets_baseline, horizons.keys())\n",
    "\n",
    "    # 保存指标到文件\n",
    "    metrics_file_baseline = output_dir / 'metrics_baseline_model.txt'\n",
    "    with open(metrics_file_baseline, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f\"Metrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n\")\n",
    "            f.write(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# 保存模型和标准化器\n",
    "model_file = output_dir / f'trajectory_predictor_current_{timestamp}.pth'\n",
    "scaler_file = output_dir / f'scaler_current_{timestamp}.save'\n",
    "\n",
    "torch.save(model.state_dict(), model_file)\n",
    "joblib.dump(scaler, scaler_file)\n",
    "print(f'Current model saved to {model_file}')\n",
    "print(f'Current scaler saved to {scaler_file}')\n",
    "\n",
    "model_file_baseline = output_dir / f'trajectory_predictor_baseline_{timestamp}.pth'\n",
    "scaler_file_baseline = output_dir / f'scaler_baseline_{timestamp}.save'\n",
    "\n",
    "torch.save(model_baseline.state_dict(), model_file_baseline)\n",
    "joblib.dump(scaler_baseline, scaler_file_baseline)\n",
    "print(f'Baseline model saved to {model_file_baseline}')\n",
    "print(f'Baseline scaler saved to {scaler_file_baseline}')\n",
    "\n",
    "# 后续的可视化和在转弯车辆上的评估代码保持不变\n",
    "# ...\n",
    "\n",
    "# ---------------------------------\n",
    "# 在转弯车辆上评估模型\n",
    "# ---------------------------------\n",
    "\n",
    "# 1. 筛选转弯车辆的数据\n",
    "turn_labels = ['left_turn', 'right_turn']\n",
    "turning_df = df_merged[df_merged['overall_turn_label'].isin(turn_labels)]\n",
    "\n",
    "# 2. 准备转弯车辆的输入和目标序列\n",
    "input_sequences_turn = []\n",
    "target_sequences_turn = []\n",
    "\n",
    "grouped_turning = turning_df.groupby('id')\n",
    "\n",
    "for track_id, group in grouped_turning:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  # 只取 'center_x' 和 'center_y'\n",
    "\n",
    "        input_sequences_turn.append(input_seq)\n",
    "        target_sequences_turn.append(target_seq)\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "input_sequences_turn = np.array(input_sequences_turn)\n",
    "target_sequences_turn = np.array(target_sequences_turn)\n",
    "\n",
    "# 数据标准化（使用相同的 scaler）\n",
    "numeric_feature_indices = [0, 1]  # 'center_x', 'center_y'\n",
    "\n",
    "# 缩放输入\n",
    "all_numeric_inputs_turn = input_sequences_turn[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs_turn).reshape(\n",
    "    input_sequences_turn.shape[0], input_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 缩放目标\n",
    "all_numeric_targets_turn = target_sequences_turn.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_turn = scaler.transform(all_numeric_targets_turn).reshape(\n",
    "    target_sequences_turn.shape[0], target_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "inputs_turn = torch.tensor(input_sequences_turn, dtype=torch.float32).to(device)\n",
    "targets_turn = torch.tensor(target_sequences_turn, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset_turn = TensorDataset(inputs_turn, targets_turn)\n",
    "data_loader_turn = DataLoader(dataset_turn, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. 在转弯车辆上评估当前模型\n",
    "def evaluate_model_on_turning(model, data_loader, target_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_outputs = []\n",
    "        total_targets = []\n",
    "        for batch_inputs, batch_targets in data_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            outputs = model(batch_inputs, target_len)\n",
    "            total_outputs.append(outputs.cpu())\n",
    "            total_targets.append(batch_targets.cpu())\n",
    "\n",
    "        total_outputs = torch.cat(total_outputs, dim=0)\n",
    "        total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "        metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "    return metrics\n",
    "\n",
    "metrics_turn_current = evaluate_model_on_turning(model, data_loader_turn, target_len)\n",
    "\n",
    "# 4. 为基线模型准备转弯车辆的数据\n",
    "# 移除转弯特征\n",
    "input_sequences_turn_baseline = input_sequences_turn[:, :, :2]  # 只保留 'center_x' 和 'center_y'\n",
    "\n",
    "# 使用基线模型的 scaler 缩放输入\n",
    "all_numeric_inputs_turn_baseline = input_sequences_turn_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn_baseline = scaler_baseline.transform(all_numeric_inputs_turn_baseline).reshape(\n",
    "    input_sequences_turn_baseline.shape[0], input_sequences_turn_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 使用基线模型的 scaler 缩放目标\n",
    "all_numeric_targets_turn_baseline = target_sequences_turn.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_turn_baseline = scaler_baseline.transform(all_numeric_targets_turn_baseline).reshape(\n",
    "    target_sequences_turn_baseline.shape[0], target_sequences_turn_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "inputs_turn_baseline = torch.tensor(input_sequences_turn_baseline, dtype=torch.float32).to(device)\n",
    "targets_turn_baseline = torch.tensor(target_sequences_turn_baseline, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset_turn_baseline = TensorDataset(inputs_turn_baseline, targets_turn_baseline)\n",
    "data_loader_turn_baseline = DataLoader(dataset_turn_baseline, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 在转弯车辆上评估基线模型\n",
    "metrics_turn_baseline = evaluate_model_on_turning(model_baseline, data_loader_turn_baseline, target_len)\n",
    "\n",
    "# 5. 比较结果\n",
    "metrics_file_turn = output_dir / 'metrics_turning_vehicles.txt'\n",
    "with open(metrics_file_turn, 'w') as f:\n",
    "    for horizon_frames, time_sec in horizons.items():\n",
    "        print(f'\\nMetrics on turning vehicles at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "        print(f\"Baseline Model:\")\n",
    "        print(f\"  RMSE: {metrics_turn_baseline[horizon_frames]['RMSE']:.4f}\")\n",
    "        print(f\"  MAE: {metrics_turn_baseline[horizon_frames]['MAE']:.4f}\")\n",
    "        print(f\"  ADE: {metrics_turn_baseline[horizon_frames]['ADE']:.4f}\")\n",
    "        print(f\"  FDE: {metrics_turn_baseline[horizon_frames]['FDE']:.4f}\")\n",
    "        print(f\"Current Model:\")\n",
    "        print(f\"  RMSE: {metrics_turn_current[horizon_frames]['RMSE']:.4f}\")\n",
    "        print(f\"  MAE: {metrics_turn_current[horizon_frames]['MAE']:.4f}\")\n",
    "        print(f\"  ADE: {metrics_turn_current[horizon_frames]['ADE']:.4f}\")\n",
    "        print(f\"  FDE: {metrics_turn_current[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "        f.write(f'Metrics on turning vehicles at horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "        f.write(f\"Baseline Model:\\n\")\n",
    "        f.write(f\"  RMSE: {metrics_turn_baseline[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "        f.write(f\"  MAE: {metrics_turn_baseline[horizon_frames]['MAE']:.4f}\\n\")\n",
    "        f.write(f\"  ADE: {metrics_turn_baseline[horizon_frames]['ADE']:.4f}\\n\")\n",
    "        f.write(f\"  FDE: {metrics_turn_baseline[horizon_frames]['FDE']:.4f}\\n\")\n",
    "        f.write(f\"Current Model:\\n\")\n",
    "        f.write(f\"  RMSE: {metrics_turn_current[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "        f.write(f\"  MAE: {metrics_turn_current[horizon_frames]['MAE']:.4f}\\n\")\n",
    "        f.write(f\"  ADE: {metrics_turn_current[horizon_frames]['ADE']:.4f}\\n\")\n",
    "        f.write(f\"  FDE: {metrics_turn_current[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "print(f\"Metrics on turning vehicles saved to {metrics_file_turn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89661b-e1be-4a5c-90bb-b631de538c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (drone_detection)",
   "language": "python",
   "name": "drone_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
