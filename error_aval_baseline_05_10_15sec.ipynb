{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4bb81b-a80c-4e9e-88a5-c51f23f34456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: eval_model_on_video/output_20241018_074844\n",
      "Number of missing 'overall_turn_label': 0\n",
      "Using device: cuda\n",
      "Total number of samples: 239560\n",
      "Number of batches per epoch: 234\n",
      "Starting epoch 1/10 for current model\n",
      "Epoch [1/10] for current model, Average Loss: 0.1051\n",
      "Starting epoch 2/10 for current model\n",
      "Epoch [2/10] for current model, Average Loss: 0.0057\n",
      "Starting epoch 3/10 for current model\n",
      "Epoch [3/10] for current model, Average Loss: 0.0040\n",
      "Starting epoch 4/10 for current model\n",
      "Epoch [4/10] for current model, Average Loss: 0.0032\n",
      "Starting epoch 5/10 for current model\n",
      "Epoch [5/10] for current model, Average Loss: 0.0029\n",
      "Starting epoch 6/10 for current model\n",
      "Epoch [6/10] for current model, Average Loss: 0.0018\n",
      "Starting epoch 7/10 for current model\n",
      "Epoch [7/10] for current model, Average Loss: 0.0014\n",
      "Starting epoch 8/10 for current model\n",
      "Epoch [8/10] for current model, Average Loss: 0.0012\n",
      "Starting epoch 9/10 for current model\n",
      "Epoch [9/10] for current model, Average Loss: 0.0010\n",
      "Starting epoch 10/10 for current model\n",
      "Epoch [10/10] for current model, Average Loss: 0.0009\n",
      "Total number of samples for baseline: 239560\n",
      "Number of batches per epoch for baseline: 234\n",
      "Starting epoch 1/10 for baseline model\n",
      "Epoch [1/10] for baseline model, Average Loss: 0.1189\n",
      "Starting epoch 2/10 for baseline model\n",
      "Epoch [2/10] for baseline model, Average Loss: 0.0045\n",
      "Starting epoch 3/10 for baseline model\n",
      "Epoch [3/10] for baseline model, Average Loss: 0.0029\n",
      "Starting epoch 4/10 for baseline model\n",
      "Epoch [4/10] for baseline model, Average Loss: 0.0024\n",
      "Starting epoch 5/10 for baseline model\n",
      "Epoch [5/10] for baseline model, Average Loss: 0.0020\n",
      "Starting epoch 6/10 for baseline model\n",
      "Epoch [6/10] for baseline model, Average Loss: 0.0018\n",
      "Starting epoch 7/10 for baseline model\n",
      "Epoch [7/10] for baseline model, Average Loss: 0.0016\n",
      "Starting epoch 8/10 for baseline model\n",
      "Epoch [8/10] for baseline model, Average Loss: 0.0015\n",
      "Starting epoch 9/10 for baseline model\n",
      "Epoch [9/10] for baseline model, Average Loss: 0.0012\n",
      "Starting epoch 10/10 for baseline model\n",
      "Epoch [10/10] for baseline model, Average Loss: 0.0010\n",
      "\n",
      "Metrics for current model at horizon: 0.5 seconds (15 frames)\n",
      "RMSE: 0.0252\n",
      "MAE: 0.0147\n",
      "ADE: 0.0234\n",
      "FDE: 0.0233\n",
      "\n",
      "Metrics for current model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0240\n",
      "MAE: 0.0160\n",
      "ADE: 0.0250\n",
      "FDE: 0.0319\n",
      "\n",
      "Metrics for current model at horizon: 1.5 seconds (45 frames)\n",
      "RMSE: 0.0298\n",
      "MAE: 0.0193\n",
      "ADE: 0.0303\n",
      "FDE: 0.0493\n",
      "\n",
      "Metrics for baseline model at horizon: 0.5 seconds (15 frames)\n",
      "RMSE: 0.0253\n",
      "MAE: 0.0145\n",
      "ADE: 0.0231\n",
      "FDE: 0.0229\n",
      "\n",
      "Metrics for baseline model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0241\n",
      "MAE: 0.0157\n",
      "ADE: 0.0250\n",
      "FDE: 0.0326\n",
      "\n",
      "Metrics for baseline model at horizon: 1.5 seconds (45 frames)\n",
      "RMSE: 0.0303\n",
      "MAE: 0.0193\n",
      "ADE: 0.0308\n",
      "FDE: 0.0514\n",
      "Current model saved to eval_model_on_video/output_20241018_074844/trajectory_predictor_current_20241018_074844.pth\n",
      "Current scaler saved to eval_model_on_video/output_20241018_074844/scaler_current_20241018_074844.save\n",
      "Baseline model saved to eval_model_on_video/output_20241018_074844/trajectory_predictor_baseline_20241018_074844.pth\n",
      "Baseline scaler saved to eval_model_on_video/output_20241018_074844/scaler_baseline_20241018_074844.save\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Current_Model_vehicle_185_seq_172706_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Current_Model_vehicle_220_seq_188381_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Current_Model_vehicle_278_seq_202552_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Current_Model_vehicle_328_seq_230159_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Current_Model_vehicle_93_seq_138782_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Baseline_Model_vehicle_185_seq_173087_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Baseline_Model_vehicle_278_seq_202260_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Baseline_Model_vehicle_278_seq_202360_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Baseline_Model_vehicle_278_seq_202669_20241018_074844.png\n",
      "Plot saved to eval_model_on_video/output_20241018_074844/Baseline_Model_vehicle_238_seq_196797_20241018_074844.png\n",
      "Metrics comparison saved to eval_model_on_video/output_20241018_074844/metrics_comparison.csv\n",
      "\n",
      "Comparison of Performance Metrics:\n",
      "   Horizon (s)  Horizon (frames)     Model      RMSE       MAE       ADE  \\\n",
      "0          0.5                15  Baseline  0.025331  0.014471  0.023095   \n",
      "1          0.5                15   Current  0.025172  0.014721  0.023404   \n",
      "2          1.0                30  Baseline  0.024112  0.015657  0.024951   \n",
      "3          1.0                30   Current  0.023965  0.015956  0.024998   \n",
      "4          1.5                45  Baseline  0.030272  0.019332  0.030759   \n",
      "5          1.5                45   Current  0.029824  0.019296  0.030322   \n",
      "\n",
      "        FDE  \n",
      "0  0.022886  \n",
      "1  0.023341  \n",
      "2  0.032596  \n",
      "3  0.031891  \n",
      "4  0.051385  \n",
      "5  0.049344  \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "path = Path('csv_out')\n",
    "eval_video_path = Path('eval_model_on_video')\n",
    "\n",
    "# Create new output directory with timestamp to avoid overwriting\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = eval_video_path / f'output_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Outputs will be saved to: {output_dir}')\n",
    "\n",
    "# Load data\n",
    "df1 = pd.read_csv(path / 'tracking_data.csv')\n",
    "df2 = pd.read_csv(path / 'overall_turn_label.csv')\n",
    "\n",
    "# Merge dataframes\n",
    "df_merged = pd.merge(df1, df2[['id', 'frame', 'overall_turn_label']], on=['id', 'frame'], how='left')\n",
    "\n",
    "# Fill missing 'overall_turn_label' values by forward and backward filling per vehicle id\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='ffill')\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='bfill')\n",
    "\n",
    "# Check for remaining missing values\n",
    "missing_values = df_merged['overall_turn_label'].isnull().sum()\n",
    "print(f\"Number of missing 'overall_turn_label': {missing_values}\")\n",
    "\n",
    "# Fill any remaining missing values with 'straight'\n",
    "df_merged['overall_turn_label'] = df_merged['overall_turn_label'].fillna('straight')\n",
    "\n",
    "# One-Hot encode 'overall_turn_label'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turn_labels_encoded = encoder.fit_transform(df_merged[['overall_turn_label']])\n",
    "turn_label_columns = encoder.get_feature_names_out(['overall_turn_label'])\n",
    "df_merged[turn_label_columns] = turn_labels_encoded\n",
    "\n",
    "# Define input features\n",
    "input_features = ['center_x', 'center_y'] + list(turn_label_columns)\n",
    "\n",
    "# Define sequence lengths\n",
    "sequence_length = 90  # Input sequence length (90 frames, 3 seconds at 30 fps)\n",
    "predict_length = 45   # Prediction sequence length (45 frames, 1.5 seconds at 30 fps)\n",
    "\n",
    "# Generate input and target sequences for the current model (with turn feature)\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "sequence_vehicle_ids = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  # Only 'center_x' and 'center_y'\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "        sequence_vehicle_ids.append(track_id)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "sequence_vehicle_ids = np.array(sequence_vehicle_ids)\n",
    "\n",
    "# Data normalization\n",
    "numeric_feature_indices = [0, 1]  # Indices for 'center_x' and 'center_y'\n",
    "\n",
    "# Flatten the inputs for scaling\n",
    "all_numeric_inputs = input_sequences[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_numeric_inputs)\n",
    "\n",
    "# Scale inputs\n",
    "input_sequences[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs).reshape(\n",
    "    input_sequences.shape[0], input_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets\n",
    "all_numeric_targets = target_sequences.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences = scaler.transform(all_numeric_targets).reshape(\n",
    "    target_sequences.shape[0], target_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Prepare baseline data (without turn feature)\n",
    "input_features_baseline = ['center_x', 'center_y']\n",
    "\n",
    "input_sequences_baseline = []\n",
    "target_sequences_baseline = []\n",
    "sequence_vehicle_ids_baseline = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features_baseline].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]\n",
    "\n",
    "        input_sequences_baseline.append(input_seq)\n",
    "        target_sequences_baseline.append(target_seq)\n",
    "        sequence_vehicle_ids_baseline.append(track_id)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "input_sequences_baseline = np.array(input_sequences_baseline)\n",
    "target_sequences_baseline = np.array(target_sequences_baseline)\n",
    "sequence_vehicle_ids_baseline = np.array(sequence_vehicle_ids_baseline)\n",
    "\n",
    "# Data normalization for baseline\n",
    "all_numeric_inputs_baseline = input_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler_baseline = StandardScaler()\n",
    "scaler_baseline.fit(all_numeric_inputs_baseline)\n",
    "\n",
    "# Scale inputs\n",
    "input_sequences_baseline = scaler_baseline.transform(all_numeric_inputs_baseline).reshape(\n",
    "    input_sequences_baseline.shape[0], input_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets\n",
    "all_numeric_targets_baseline = target_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_baseline = scaler_baseline.transform(all_numeric_targets_baseline).reshape(\n",
    "    target_sequences_baseline.shape[0], target_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Define the model\n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=2):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        self.lstm_encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_decoder = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, target_len):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Encoder\n",
    "        _, (hidden, cell) = self.lstm_encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_input = x[:, -1, :2].unsqueeze(1)  # Start with the last position of the input sequence\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            out, (hidden, cell) = self.lstm_decoder(decoder_input, (hidden, cell))\n",
    "            out = self.fc_out(out)\n",
    "            outputs.append(out.squeeze(1))\n",
    "            decoder_input = out  # Use current output as next input\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 1024\n",
    "num_epochs = 10\n",
    "target_len = predict_length  # Prediction length\n",
    "\n",
    "# Prepare data for current model\n",
    "input_size = input_sequences.shape[2]  # Number of input features\n",
    "inputs = torch.tensor(input_sequences, dtype=torch.float32).to(device)\n",
    "targets = torch.tensor(target_sequences, dtype=torch.float32).to(device)\n",
    "model = TrajectoryPredictor(input_size=input_size, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "print(f'Number of batches per epoch: {len(data_loader)}')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for current model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for current model')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for current model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# Prepare data for baseline model\n",
    "input_size_baseline = input_sequences_baseline.shape[2]  # Number of input features (without turn feature)\n",
    "inputs_baseline = torch.tensor(input_sequences_baseline, dtype=torch.float32).to(device)\n",
    "targets_baseline = torch.tensor(target_sequences_baseline, dtype=torch.float32).to(device)\n",
    "model_baseline = TrajectoryPredictor(input_size=input_size_baseline, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset_baseline = TensorDataset(inputs_baseline, targets_baseline)\n",
    "data_loader_baseline = DataLoader(dataset_baseline, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples for baseline: {len(dataset_baseline)}')\n",
    "print(f'Number of batches per epoch for baseline: {len(data_loader_baseline)}')\n",
    "\n",
    "# Define loss function and optimizer for baseline model\n",
    "criterion_baseline = nn.MSELoss()\n",
    "optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for baseline model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for baseline model')\n",
    "    model_baseline.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer_baseline.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion_baseline(outputs, batch_targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer_baseline.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader_baseline)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for baseline model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# Define function to compute metrics\n",
    "def compute_metrics(predictions, targets, horizons):\n",
    "    metrics = {}\n",
    "    for horizon in horizons:\n",
    "        outputs_at_horizon = predictions[:, :horizon, :]  # [num_samples, horizon, 2]\n",
    "        targets_at_horizon = targets[:, :horizon, :]\n",
    "\n",
    "        # Compute errors\n",
    "        errors = outputs_at_horizon - targets_at_horizon  # [num_samples, horizon, 2]\n",
    "        squared_errors = errors ** 2\n",
    "        mse = squared_errors.mean().item()\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        abs_errors = errors.abs()\n",
    "        mae = abs_errors.mean().item()\n",
    "\n",
    "        # Compute ADE\n",
    "        displacement_errors = torch.norm(errors, dim=2)  # Euclidean distance over x and y\n",
    "        ade = displacement_errors.mean().item()\n",
    "\n",
    "        # Compute FDE\n",
    "        final_errors = errors[:, -1, :]  # [num_samples, 2]\n",
    "        fde = torch.norm(final_errors, dim=1).mean().item()\n",
    "\n",
    "        metrics[horizon] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'ADE': ade,\n",
    "            'FDE': fde\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Define prediction horizons\n",
    "horizons = {\n",
    "    15: 0.5,  # 0.5 seconds (15 frames)\n",
    "    30: 1.0,  # 1.0 seconds (30 frames)\n",
    "    45: 1.5   # 1.5 seconds (45 frames)\n",
    "}\n",
    "\n",
    "# Filter horizons not exceeding predict_length\n",
    "horizons = {k: v for k, v in horizons.items() if k <= predict_length}\n",
    "\n",
    "# Evaluate current model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs = []\n",
    "    total_targets = []\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "        total_outputs.append(outputs.cpu())\n",
    "        total_targets.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs = torch.cat(total_outputs, dim=0)\n",
    "    total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "    metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "\n",
    "    # Save metrics to file\n",
    "    metrics_file = output_dir / 'metrics_current_model.txt'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f'Metrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "            f.write(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# Evaluate baseline model\n",
    "model_baseline.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs_baseline = []\n",
    "    total_targets_baseline = []\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "        total_outputs_baseline.append(outputs.cpu())\n",
    "        total_targets_baseline.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs_baseline = torch.cat(total_outputs_baseline, dim=0)\n",
    "    total_targets_baseline = torch.cat(total_targets_baseline, dim=0)\n",
    "\n",
    "    metrics_baseline = compute_metrics(total_outputs_baseline, total_targets_baseline, horizons.keys())\n",
    "\n",
    "    # Save metrics to file\n",
    "    metrics_file_baseline = output_dir / 'metrics_baseline_model.txt'\n",
    "    with open(metrics_file_baseline, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            # Corrected the mismatched quotation mark\n",
    "            f.write(f\"Metrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n\")\n",
    "            f.write(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# Save models and scalers\n",
    "model_file = output_dir / f'trajectory_predictor_current_{timestamp}.pth'\n",
    "scaler_file = output_dir / f'scaler_current_{timestamp}.save'\n",
    "\n",
    "torch.save(model.state_dict(), model_file)\n",
    "joblib.dump(scaler, scaler_file)\n",
    "print(f'Current model saved to {model_file}')\n",
    "print(f'Current scaler saved to {scaler_file}')\n",
    "\n",
    "model_file_baseline = output_dir / f'trajectory_predictor_baseline_{timestamp}.pth'\n",
    "scaler_file_baseline = output_dir / f'scaler_baseline_{timestamp}.save'\n",
    "\n",
    "torch.save(model_baseline.state_dict(), model_file_baseline)\n",
    "joblib.dump(scaler_baseline, scaler_file_baseline)\n",
    "print(f'Baseline model saved to {model_file_baseline}')\n",
    "print(f'Baseline scaler saved to {scaler_file_baseline}')\n",
    "\n",
    "# Visualization (optional)\n",
    "vehicle_ids_of_interest = [50, 328, 220, 46, 201, 238, 278, 185, 309, 303, 74, 93, 127, 203, 219, 210, 280, 390]\n",
    "\n",
    "# Create vehicle ID to indices mapping for current model\n",
    "vehicle_id_to_indices_current = {}\n",
    "for vehicle_id in vehicle_ids_of_interest:\n",
    "    indices = np.where(sequence_vehicle_ids == vehicle_id)[0]\n",
    "    if len(indices) > 0:\n",
    "        vehicle_id_to_indices_current[vehicle_id] = indices\n",
    "    else:\n",
    "        print(f\"Vehicle ID {vehicle_id} not found in the current model sequences.\")\n",
    "\n",
    "# Create vehicle ID to indices mapping for baseline model\n",
    "vehicle_id_to_indices_baseline = {}\n",
    "for vehicle_id in vehicle_ids_of_interest:\n",
    "    indices = np.where(sequence_vehicle_ids_baseline == vehicle_id)[0]\n",
    "    if len(indices) > 0:\n",
    "        vehicle_id_to_indices_baseline[vehicle_id] = indices\n",
    "    else:\n",
    "        print(f\"Vehicle ID {vehicle_id} not found in the baseline model sequences.\")\n",
    "\n",
    "# Define visualization function\n",
    "def visualize_prediction(model, inputs, targets, scaler, vehicle_id_to_indices, model_name):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create index to vehicle ID mapping\n",
    "        index_to_vehicle_id = {}\n",
    "        for vehicle_id, indices in vehicle_id_to_indices.items():\n",
    "            for idx in indices:\n",
    "                index_to_vehicle_id[idx] = vehicle_id\n",
    "\n",
    "        # Collect all indices\n",
    "        all_indices = list(index_to_vehicle_id.keys())\n",
    "\n",
    "        # Randomly select 5 indices\n",
    "        num_samples = 5\n",
    "        if len(all_indices) >= num_samples:\n",
    "            selected_indices = random.sample(all_indices, num_samples)\n",
    "        else:\n",
    "            selected_indices = all_indices  # If less than 5 sequences are available\n",
    "\n",
    "        for idx in selected_indices:\n",
    "            test_input = inputs[idx].unsqueeze(0).to(device)\n",
    "            true_target = targets[idx].to(device)\n",
    "\n",
    "            # Perform prediction\n",
    "            predicted_output = model(test_input, target_len)\n",
    "\n",
    "            # Convert predictions and true targets to NumPy arrays\n",
    "            predicted_output = predicted_output.squeeze(0).cpu().numpy()\n",
    "            true_target = true_target.cpu().numpy()\n",
    "\n",
    "            # Get historical input data for visualization\n",
    "            history_input = test_input.squeeze(0).cpu().numpy()\n",
    "\n",
    "            # Inverse scaling\n",
    "            numeric_feature_indices = [0, 1]  # Indices of 'center_x' and 'center_y'\n",
    "\n",
    "            # Inverse transform historical inputs\n",
    "            history_input_numeric = history_input[:, numeric_feature_indices]\n",
    "            history_input_unscaled = scaler.inverse_transform(history_input_numeric)\n",
    "\n",
    "            # Inverse transform predicted outputs\n",
    "            predicted_output_unscaled = scaler.inverse_transform(predicted_output)\n",
    "\n",
    "            # Inverse transform true targets\n",
    "            true_target_unscaled = scaler.inverse_transform(true_target)\n",
    "\n",
    "            # Visualization\n",
    "            plt.figure(figsize=(8, 6))\n",
    "\n",
    "            # Plot historical trajectory\n",
    "            plt.plot(history_input_unscaled[:, 0], history_input_unscaled[:, 1], 'bo-', label='Historical Trajectory')\n",
    "\n",
    "            # Plot true future trajectory\n",
    "            plt.plot(true_target_unscaled[:, 0], true_target_unscaled[:, 1], 'go-', label='True Future Trajectory')\n",
    "\n",
    "            # Plot predicted future trajectory\n",
    "            plt.plot(predicted_output_unscaled[:, 0], predicted_output_unscaled[:, 1], 'ro--', label='Predicted Future Trajectory')\n",
    "\n",
    "            plt.legend()\n",
    "            plt.xlabel('center_x')\n",
    "            plt.ylabel('center_y')\n",
    "            vehicle_id = index_to_vehicle_id.get(idx, 'Unknown')\n",
    "            plt.title(f'{model_name} - Vehicle {vehicle_id} Trajectory Prediction (Seq {idx})')\n",
    "\n",
    "            # Save figure\n",
    "            figure_path = output_dir / f'{model_name}_vehicle_{vehicle_id}_seq_{idx}_{timestamp}.png'\n",
    "            plt.savefig(figure_path)\n",
    "            plt.close()\n",
    "            print(f'Plot saved to {figure_path}')\n",
    "\n",
    "# Visualize predictions for current model\n",
    "visualize_prediction(model, inputs.cpu(), targets.cpu(), scaler, vehicle_id_to_indices_current, model_name='Current_Model')\n",
    "\n",
    "# Visualize predictions for baseline model\n",
    "visualize_prediction(model_baseline, inputs_baseline.cpu(), targets_baseline.cpu(), scaler_baseline, vehicle_id_to_indices_baseline, model_name='Baseline_Model')\n",
    "\n",
    "# Combine metrics from both models into a DataFrame and save\n",
    "metrics_combined = []\n",
    "\n",
    "for horizon_frames, time_sec in horizons.items():\n",
    "    metrics_combined.append({\n",
    "        'Horizon (s)': time_sec,\n",
    "        'Horizon (frames)': horizon_frames,\n",
    "        'Model': 'Baseline',\n",
    "        'RMSE': metrics_baseline[horizon_frames]['RMSE'],\n",
    "        'MAE': metrics_baseline[horizon_frames]['MAE'],\n",
    "        'ADE': metrics_baseline[horizon_frames]['ADE'],\n",
    "        'FDE': metrics_baseline[horizon_frames]['FDE']\n",
    "    })\n",
    "    metrics_combined.append({\n",
    "        'Horizon (s)': time_sec,\n",
    "        'Horizon (frames)': horizon_frames,\n",
    "        'Model': 'Current',\n",
    "        'RMSE': metrics[horizon_frames]['RMSE'],\n",
    "        'MAE': metrics[horizon_frames]['MAE'],\n",
    "        'ADE': metrics[horizon_frames]['ADE'],\n",
    "        'FDE': metrics[horizon_frames]['FDE']\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_combined)\n",
    "metrics_csv_file = output_dir / 'metrics_comparison.csv'\n",
    "df_metrics.to_csv(metrics_csv_file, index=False)\n",
    "print(f'Metrics comparison saved to {metrics_csv_file}')\n",
    "\n",
    "# Print comparison of performance metrics\n",
    "print(\"\\nComparison of Performance Metrics:\")\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6485810-8dbc-4663-851f-e3cba773b579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: eval_model_on_video/output_20241018_080535\n",
      "Number of missing 'overall_turn_label': 0\n",
      "Using device: cuda\n",
      "Total number of samples: 239560\n",
      "Number of batches per epoch: 234\n",
      "Starting epoch 1/10 for current model\n",
      "Epoch [1/10] for current model, Average Loss: 0.1196\n",
      "Starting epoch 2/10 for current model\n",
      "Epoch [2/10] for current model, Average Loss: 0.0049\n",
      "Starting epoch 3/10 for current model\n",
      "Epoch [3/10] for current model, Average Loss: 0.0034\n",
      "Starting epoch 4/10 for current model\n",
      "Epoch [4/10] for current model, Average Loss: 0.0026\n",
      "Starting epoch 5/10 for current model\n",
      "Epoch [5/10] for current model, Average Loss: 0.0021\n",
      "Starting epoch 6/10 for current model\n",
      "Epoch [6/10] for current model, Average Loss: 0.0018\n",
      "Starting epoch 7/10 for current model\n",
      "Epoch [7/10] for current model, Average Loss: 0.0015\n",
      "Starting epoch 8/10 for current model\n",
      "Epoch [8/10] for current model, Average Loss: 0.0013\n",
      "Starting epoch 9/10 for current model\n",
      "Epoch [9/10] for current model, Average Loss: 0.0012\n",
      "Starting epoch 10/10 for current model\n",
      "Epoch [10/10] for current model, Average Loss: 0.0010\n",
      "Total number of samples for baseline: 239560\n",
      "Number of batches per epoch for baseline: 234\n",
      "Starting epoch 1/10 for baseline model\n",
      "Epoch [1/10] for baseline model, Average Loss: 0.1075\n",
      "Starting epoch 2/10 for baseline model\n",
      "Epoch [2/10] for baseline model, Average Loss: 0.0049\n",
      "Starting epoch 3/10 for baseline model\n",
      "Epoch [3/10] for baseline model, Average Loss: 0.0032\n",
      "Starting epoch 4/10 for baseline model\n",
      "Epoch [4/10] for baseline model, Average Loss: 0.0029\n",
      "Starting epoch 5/10 for baseline model\n",
      "Epoch [5/10] for baseline model, Average Loss: 0.0025\n",
      "Starting epoch 6/10 for baseline model\n",
      "Epoch [6/10] for baseline model, Average Loss: 0.0021\n",
      "Starting epoch 7/10 for baseline model\n",
      "Epoch [7/10] for baseline model, Average Loss: 0.0017\n",
      "Starting epoch 8/10 for baseline model\n",
      "Epoch [8/10] for baseline model, Average Loss: 0.0013\n",
      "Starting epoch 9/10 for baseline model\n",
      "Epoch [9/10] for baseline model, Average Loss: 0.0012\n",
      "Starting epoch 10/10 for baseline model\n",
      "Epoch [10/10] for baseline model, Average Loss: 0.0011\n",
      "\n",
      "Metrics for current model at horizon: 0.5 seconds (15 frames)\n",
      "RMSE: 0.0258\n",
      "MAE: 0.0162\n",
      "ADE: 0.0259\n",
      "FDE: 0.0278\n",
      "\n",
      "Metrics for current model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0259\n",
      "MAE: 0.0184\n",
      "ADE: 0.0291\n",
      "FDE: 0.0385\n",
      "\n",
      "Metrics for current model at horizon: 1.5 seconds (45 frames)\n",
      "RMSE: 0.0325\n",
      "MAE: 0.0223\n",
      "ADE: 0.0354\n",
      "FDE: 0.0565\n",
      "\n",
      "Metrics for baseline model at horizon: 0.5 seconds (15 frames)\n",
      "RMSE: 0.0264\n",
      "MAE: 0.0154\n",
      "ADE: 0.0252\n",
      "FDE: 0.0239\n",
      "\n",
      "Metrics for baseline model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0256\n",
      "MAE: 0.0164\n",
      "ADE: 0.0268\n",
      "FDE: 0.0354\n",
      "\n",
      "Metrics for baseline model at horizon: 1.5 seconds (45 frames)\n",
      "RMSE: 0.0325\n",
      "MAE: 0.0201\n",
      "ADE: 0.0332\n",
      "FDE: 0.0557\n",
      "Current model saved to eval_model_on_video/output_20241018_080535/trajectory_predictor_current_20241018_080535.pth\n",
      "Current scaler saved to eval_model_on_video/output_20241018_080535/scaler_current_20241018_080535.save\n",
      "Baseline model saved to eval_model_on_video/output_20241018_080535/trajectory_predictor_baseline_20241018_080535.pth\n",
      "Baseline scaler saved to eval_model_on_video/output_20241018_080535/scaler_baseline_20241018_080535.save\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'target_sequences_turn_baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 488\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# Scale targets using baseline scaler\u001b[39;00m\n\u001b[1;32m    486\u001b[0m all_numeric_targets_turn_baseline \u001b[38;5;241m=\u001b[39m target_sequences_turn\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n\u001b[1;32m    487\u001b[0m target_sequences_turn_baseline \u001b[38;5;241m=\u001b[39m scaler_baseline\u001b[38;5;241m.\u001b[39mtransform(all_numeric_targets_turn_baseline)\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m--> 488\u001b[0m     \u001b[43mtarget_sequences_turn_baseline\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], target_sequences_turn_baseline\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[1;32m    491\u001b[0m inputs_turn_baseline \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_sequences_turn_baseline, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_sequences_turn_baseline' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "path = Path('csv_out')\n",
    "eval_video_path = Path('eval_model_on_video')\n",
    "\n",
    "# Create new output directory with timestamp to avoid overwriting\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = eval_video_path / f'output_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Outputs will be saved to: {output_dir}')\n",
    "\n",
    "# Load data\n",
    "df1 = pd.read_csv(path / 'tracking_data.csv')\n",
    "df2 = pd.read_csv(path / 'overall_turn_label.csv')\n",
    "\n",
    "# Merge dataframes\n",
    "df_merged = pd.merge(df1, df2[['id', 'frame', 'overall_turn_label']], on=['id', 'frame'], how='left')\n",
    "\n",
    "# Fill missing 'overall_turn_label' values by forward and backward filling per vehicle id\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='ffill')\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='bfill')\n",
    "\n",
    "# Check for remaining missing values\n",
    "missing_values = df_merged['overall_turn_label'].isnull().sum()\n",
    "print(f\"Number of missing 'overall_turn_label': {missing_values}\")\n",
    "\n",
    "# Fill any remaining missing values with 'straight' (if any)\n",
    "df_merged['overall_turn_label'] = df_merged['overall_turn_label'].fillna('straight')\n",
    "\n",
    "# One-Hot encode 'overall_turn_label'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turn_labels_encoded = encoder.fit_transform(df_merged[['overall_turn_label']])\n",
    "turn_label_columns = encoder.get_feature_names_out(['overall_turn_label'])\n",
    "df_merged[turn_label_columns] = turn_labels_encoded\n",
    "\n",
    "# Define input features\n",
    "input_features = ['center_x', 'center_y'] + list(turn_label_columns)\n",
    "\n",
    "# Define sequence lengths\n",
    "sequence_length = 90  # Input sequence length (90 frames, 3 seconds at 30 fps)\n",
    "predict_length = 45   # Prediction sequence length (45 frames, 1.5 seconds at 30 fps)\n",
    "\n",
    "# Generate input and target sequences for the current model (with turn feature)\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "sequence_vehicle_ids = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  # Only 'center_x' and 'center_y'\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "        sequence_vehicle_ids.append(track_id)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "sequence_vehicle_ids = np.array(sequence_vehicle_ids)\n",
    "\n",
    "# Data normalization\n",
    "numeric_feature_indices = [0, 1]  # Indices for 'center_x' and 'center_y'\n",
    "\n",
    "# Flatten the inputs for scaling\n",
    "all_numeric_inputs = input_sequences[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_numeric_inputs)\n",
    "\n",
    "# Scale inputs\n",
    "input_sequences[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs).reshape(\n",
    "    input_sequences.shape[0], input_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets\n",
    "all_numeric_targets = target_sequences.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences = scaler.transform(all_numeric_targets).reshape(\n",
    "    target_sequences.shape[0], target_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Prepare baseline data (without turn feature)\n",
    "input_features_baseline = ['center_x', 'center_y']\n",
    "\n",
    "input_sequences_baseline = []\n",
    "target_sequences_baseline = []\n",
    "sequence_vehicle_ids_baseline = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features_baseline].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]\n",
    "\n",
    "        input_sequences_baseline.append(input_seq)\n",
    "        target_sequences_baseline.append(target_seq)\n",
    "        sequence_vehicle_ids_baseline.append(track_id)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "input_sequences_baseline = np.array(input_sequences_baseline)\n",
    "target_sequences_baseline = np.array(target_sequences_baseline)\n",
    "sequence_vehicle_ids_baseline = np.array(sequence_vehicle_ids_baseline)\n",
    "\n",
    "# Data normalization for baseline\n",
    "all_numeric_inputs_baseline = input_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler_baseline = StandardScaler()\n",
    "scaler_baseline.fit(all_numeric_inputs_baseline)\n",
    "\n",
    "# Scale inputs\n",
    "input_sequences_baseline = scaler_baseline.transform(all_numeric_inputs_baseline).reshape(\n",
    "    input_sequences_baseline.shape[0], input_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets\n",
    "all_numeric_targets_baseline = target_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_baseline = scaler_baseline.transform(all_numeric_targets_baseline).reshape(\n",
    "    target_sequences_baseline.shape[0], target_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Define the model\n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=2):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        self.lstm_encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_decoder = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, target_len):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Encoder\n",
    "        _, (hidden, cell) = self.lstm_encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_input = x[:, -1, :2].unsqueeze(1)  # Start with the last position of the input sequence\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            out, (hidden, cell) = self.lstm_decoder(decoder_input, (hidden, cell))\n",
    "            out = self.fc_out(out)\n",
    "            outputs.append(out.squeeze(1))\n",
    "            decoder_input = out  # Use current output as next input\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 1024\n",
    "num_epochs = 10\n",
    "target_len = predict_length  # Prediction length\n",
    "\n",
    "# Prepare data for current model\n",
    "input_size = input_sequences.shape[2]  # Number of input features\n",
    "inputs = torch.tensor(input_sequences, dtype=torch.float32).to(device)\n",
    "targets = torch.tensor(target_sequences, dtype=torch.float32).to(device)\n",
    "model = TrajectoryPredictor(input_size=input_size, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "print(f'Number of batches per epoch: {len(data_loader)}')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for current model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for current model')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for current model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# Prepare data for baseline model\n",
    "input_size_baseline = input_sequences_baseline.shape[2]  # Number of input features (without turn feature)\n",
    "inputs_baseline = torch.tensor(input_sequences_baseline, dtype=torch.float32).to(device)\n",
    "targets_baseline = torch.tensor(target_sequences_baseline, dtype=torch.float32).to(device)\n",
    "model_baseline = TrajectoryPredictor(input_size=input_size_baseline, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset_baseline = TensorDataset(inputs_baseline, targets_baseline)\n",
    "data_loader_baseline = DataLoader(dataset_baseline, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples for baseline: {len(dataset_baseline)}')\n",
    "print(f'Number of batches per epoch for baseline: {len(data_loader_baseline)}')\n",
    "\n",
    "# Define loss function and optimizer for baseline model\n",
    "criterion_baseline = nn.MSELoss()\n",
    "optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for baseline model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for baseline model')\n",
    "    model_baseline.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer_baseline.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion_baseline(outputs, batch_targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer_baseline.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader_baseline)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for baseline model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# Define function to compute metrics\n",
    "def compute_metrics(predictions, targets, horizons):\n",
    "    metrics = {}\n",
    "    for horizon in horizons:\n",
    "        outputs_at_horizon = predictions[:, :horizon, :]  # [num_samples, horizon, 2]\n",
    "        targets_at_horizon = targets[:, :horizon, :]\n",
    "\n",
    "        # Compute errors\n",
    "        errors = outputs_at_horizon - targets_at_horizon  # [num_samples, horizon, 2]\n",
    "        squared_errors = errors ** 2\n",
    "        mse = squared_errors.mean().item()\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        abs_errors = errors.abs()\n",
    "        mae = abs_errors.mean().item()\n",
    "\n",
    "        # Compute ADE\n",
    "        displacement_errors = torch.norm(errors, dim=2)  # Euclidean distance over x and y\n",
    "        ade = displacement_errors.mean().item()\n",
    "\n",
    "        # Compute FDE\n",
    "        final_errors = errors[:, -1, :]  # [num_samples, 2]\n",
    "        fde = torch.norm(final_errors, dim=1).mean().item()\n",
    "\n",
    "        metrics[horizon] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'ADE': ade,\n",
    "            'FDE': fde\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Define prediction horizons\n",
    "horizons = {\n",
    "    15: 0.5,  # 0.5 seconds (15 frames)\n",
    "    30: 1.0,  # 1.0 seconds (30 frames)\n",
    "    45: 1.5   # 1.5 seconds (45 frames)\n",
    "}\n",
    "\n",
    "# Filter horizons not exceeding predict_length\n",
    "horizons = {k: v for k, v in horizons.items() if k <= predict_length}\n",
    "\n",
    "# Evaluate current model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs = []\n",
    "    total_targets = []\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "        total_outputs.append(outputs.cpu())\n",
    "        total_targets.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs = torch.cat(total_outputs, dim=0)\n",
    "    total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "    metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "\n",
    "    # Save metrics to file\n",
    "    metrics_file = output_dir / 'metrics_current_model.txt'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f'Metrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "            f.write(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# Evaluate baseline model\n",
    "model_baseline.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs_baseline = []\n",
    "    total_targets_baseline = []\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "        total_outputs_baseline.append(outputs.cpu())\n",
    "        total_targets_baseline.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs_baseline = torch.cat(total_outputs_baseline, dim=0)\n",
    "    total_targets_baseline = torch.cat(total_targets_baseline, dim=0)\n",
    "\n",
    "    metrics_baseline = compute_metrics(total_outputs_baseline, total_targets_baseline, horizons.keys())\n",
    "\n",
    "    # Save metrics to file\n",
    "    metrics_file_baseline = output_dir / 'metrics_baseline_model.txt'\n",
    "    with open(metrics_file_baseline, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f\"Metrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n\")\n",
    "            f.write(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# Save models and scalers\n",
    "model_file = output_dir / f'trajectory_predictor_current_{timestamp}.pth'\n",
    "scaler_file = output_dir / f'scaler_current_{timestamp}.save'\n",
    "\n",
    "torch.save(model.state_dict(), model_file)\n",
    "joblib.dump(scaler, scaler_file)\n",
    "print(f'Current model saved to {model_file}')\n",
    "print(f'Current scaler saved to {scaler_file}')\n",
    "\n",
    "model_file_baseline = output_dir / f'trajectory_predictor_baseline_{timestamp}.pth'\n",
    "scaler_file_baseline = output_dir / f'scaler_baseline_{timestamp}.save'\n",
    "\n",
    "torch.save(model_baseline.state_dict(), model_file_baseline)\n",
    "joblib.dump(scaler_baseline, scaler_file_baseline)\n",
    "print(f'Baseline model saved to {model_file_baseline}')\n",
    "print(f'Baseline scaler saved to {scaler_file_baseline}')\n",
    "\n",
    "# Visualization and metrics comparison code remains the same\n",
    "# ...\n",
    "\n",
    "# ---------------------------------\n",
    "# Evaluate models on turning vehicles\n",
    "# ---------------------------------\n",
    "\n",
    "# 1. Filter data for turning vehicles based on 'overall_turn_label'\n",
    "turn_labels = ['left_turn', 'right_turn']\n",
    "\n",
    "# Since your 'overall_turn_label' represents the true turning status, use it directly\n",
    "turning_df = df_merged[df_merged['overall_turn_label'].isin(turn_labels)]\n",
    "\n",
    "# 2. Prepare sequences for turning vehicles for current model\n",
    "input_sequences_turn = []\n",
    "target_sequences_turn = []\n",
    "\n",
    "grouped_turning = turning_df.groupby('id')\n",
    "\n",
    "for track_id, group in grouped_turning:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  # Only 'center_x' and 'center_y'\n",
    "\n",
    "        input_sequences_turn.append(input_seq)\n",
    "        target_sequences_turn.append(target_seq)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "input_sequences_turn = np.array(input_sequences_turn)\n",
    "target_sequences_turn = np.array(target_sequences_turn)\n",
    "\n",
    "# Data normalization (using the same scaler)\n",
    "numeric_feature_indices = [0, 1]  # 'center_x', 'center_y'\n",
    "\n",
    "# Scale inputs\n",
    "all_numeric_inputs_turn = input_sequences_turn[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs_turn).reshape(\n",
    "    input_sequences_turn.shape[0], input_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets\n",
    "all_numeric_targets_turn = target_sequences_turn.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_turn = scaler.transform(all_numeric_targets_turn).reshape(\n",
    "    target_sequences_turn.shape[0], target_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Create dataset and dataloader\n",
    "inputs_turn = torch.tensor(input_sequences_turn, dtype=torch.float32).to(device)\n",
    "targets_turn = torch.tensor(target_sequences_turn, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset_turn = TensorDataset(inputs_turn, targets_turn)\n",
    "data_loader_turn = DataLoader(dataset_turn, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. Evaluate current model on turning vehicles\n",
    "def evaluate_model_on_turning(model, data_loader, target_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_outputs = []\n",
    "        total_targets = []\n",
    "        for batch_inputs, batch_targets in data_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            outputs = model(batch_inputs, target_len)\n",
    "            total_outputs.append(outputs.cpu())\n",
    "            total_targets.append(batch_targets.cpu())\n",
    "\n",
    "        total_outputs = torch.cat(total_outputs, dim=0)\n",
    "        total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "        metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "    return metrics\n",
    "\n",
    "metrics_turn_current = evaluate_model_on_turning(model, data_loader_turn, target_len)\n",
    "\n",
    "# 4. Prepare data for baseline model on turning vehicles\n",
    "# Remove turn features\n",
    "input_sequences_turn_baseline = input_sequences_turn[:, :, :2]  # Only 'center_x' and 'center_y'\n",
    "\n",
    "# Scale inputs using baseline scaler\n",
    "all_numeric_inputs_turn_baseline = input_sequences_turn_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn_baseline = scaler_baseline.transform(all_numeric_inputs_turn_baseline).reshape(\n",
    "    input_sequences_turn_baseline.shape[0], input_sequences_turn_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Scale targets using baseline scaler\n",
    "all_numeric_targets_turn_baseline = target_sequences_turn.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_turn_baseline = scaler_baseline.transform(all_numeric_targets_turn_baseline).reshape(\n",
    "    target_sequences_turn_baseline.shape[0], target_sequences_turn_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# Create dataset and dataloader\n",
    "inputs_turn_baseline = torch.tensor(input_sequences_turn_baseline, dtype=torch.float32).to(device)\n",
    "targets_turn_baseline = torch.tensor(target_sequences_turn_baseline, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset_turn_baseline = TensorDataset(inputs_turn_baseline, targets_turn_baseline)\n",
    "data_loader_turn_baseline = DataLoader(dataset_turn_baseline, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate baseline model on turning vehicles\n",
    "metrics_turn_baseline = evaluate_model_on_turning(model_baseline, data_loader_turn_baseline, target_len)\n",
    "\n",
    "# 5. Compare results\n",
    "metrics_file_turn = output_dir / 'metrics_turning_vehicles.txt'\n",
    "with open(metrics_file_turn, 'w') as f:\n",
    "    for horizon_frames, time_sec in horizons.items():\n",
    "        print(f'\\nMetrics on turning vehicles at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "        print(f\"Baseline Model:\")\n",
    "        print(f\"  RMSE: {metrics_turn_baseline[horizon_frames]['RMSE']:.4f}\")\n",
    "        print(f\"  MAE: {metrics_turn_baseline[horizon_frames]['MAE']:.4f}\")\n",
    "        print(f\"  ADE: {metrics_turn_baseline[horizon_frames]['ADE']:.4f}\")\n",
    "        print(f\"  FDE: {metrics_turn_baseline[horizon_frames]['FDE']:.4f}\")\n",
    "        print(f\"Current Model:\")\n",
    "        print(f\"  RMSE: {metrics_turn_current[horizon_frames]['RMSE']:.4f}\")\n",
    "        print(f\"  MAE: {metrics_turn_current[horizon_frames]['MAE']:.4f}\")\n",
    "        print(f\"  ADE: {metrics_turn_current[horizon_frames]['ADE']:.4f}\")\n",
    "        print(f\"  FDE: {metrics_turn_current[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "        f.write(f'Metrics on turning vehicles at horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "        f.write(f\"Baseline Model:\\n\")\n",
    "        f.write(f\"  RMSE: {metrics_turn_baseline[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "        f.write(f\"  MAE: {metrics_turn_baseline[horizon_frames]['MAE']:.4f}\\n\")\n",
    "        f.write(f\"  ADE: {metrics_turn_baseline[horizon_frames]['ADE']:.4f}\\n\")\n",
    "        f.write(f\"  FDE: {metrics_turn_baseline[horizon_frames]['FDE']:.4f}\\n\")\n",
    "        f.write(f\"Current Model:\\n\")\n",
    "        f.write(f\"  RMSE: {metrics_turn_current[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "        f.write(f\"  MAE: {metrics_turn_current[horizon_frames]['MAE']:.4f}\\n\")\n",
    "        f.write(f\"  ADE: {metrics_turn_current[horizon_frames]['ADE']:.4f}\\n\")\n",
    "        f.write(f\"  FDE: {metrics_turn_current[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "print(f\"Metrics on turning vehicles saved to {metrics_file_turn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e3d0b2-4a3b-42da-bccc-fbf7ce4b2edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: eval_model_on_video/output_20241018_082033\n",
      "Number of missing 'overall_turn_label': 0\n",
      "Using device: cuda\n",
      "Total number of samples: 239560\n",
      "Number of batches per epoch: 234\n",
      "Starting epoch 1/10 for current model\n",
      "Epoch [1/10] for current model, Average Loss: 0.1221\n",
      "Starting epoch 2/10 for current model\n",
      "Epoch [2/10] for current model, Average Loss: 0.0044\n",
      "Starting epoch 3/10 for current model\n",
      "Epoch [3/10] for current model, Average Loss: 0.0032\n",
      "Starting epoch 4/10 for current model\n",
      "Epoch [4/10] for current model, Average Loss: 0.0026\n",
      "Starting epoch 5/10 for current model\n",
      "Epoch [5/10] for current model, Average Loss: 0.0022\n",
      "Starting epoch 6/10 for current model\n",
      "Epoch [6/10] for current model, Average Loss: 0.0016\n",
      "Starting epoch 7/10 for current model\n",
      "Epoch [7/10] for current model, Average Loss: 0.0403\n",
      "Starting epoch 8/10 for current model\n",
      "Epoch [8/10] for current model, Average Loss: 0.0051\n",
      "Starting epoch 9/10 for current model\n",
      "Epoch [9/10] for current model, Average Loss: 0.0014\n",
      "Starting epoch 10/10 for current model\n",
      "Epoch [10/10] for current model, Average Loss: 0.0011\n",
      "Total number of samples for baseline: 239560\n",
      "Number of batches per epoch for baseline: 234\n",
      "Starting epoch 1/10 for baseline model\n",
      "Epoch [1/10] for baseline model, Average Loss: 0.1143\n",
      "Starting epoch 2/10 for baseline model\n",
      "Epoch [2/10] for baseline model, Average Loss: 0.0053\n",
      "Starting epoch 3/10 for baseline model\n",
      "Epoch [3/10] for baseline model, Average Loss: 0.0035\n",
      "Starting epoch 4/10 for baseline model\n",
      "Epoch [4/10] for baseline model, Average Loss: 0.0029\n",
      "Starting epoch 5/10 for baseline model\n",
      "Epoch [5/10] for baseline model, Average Loss: 0.0022\n",
      "Starting epoch 6/10 for baseline model\n",
      "Epoch [6/10] for baseline model, Average Loss: 0.0020\n",
      "Starting epoch 7/10 for baseline model\n",
      "Epoch [7/10] for baseline model, Average Loss: 0.0015\n",
      "Starting epoch 8/10 for baseline model\n",
      "Epoch [8/10] for baseline model, Average Loss: 0.0014\n",
      "Starting epoch 9/10 for baseline model\n",
      "Epoch [9/10] for baseline model, Average Loss: 0.0012\n",
      "Starting epoch 10/10 for baseline model\n",
      "Epoch [10/10] for baseline model, Average Loss: 0.0011\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 412\u001b[0m\n\u001b[1;32m    408\u001b[0m predictions_turn_baseline_unscaled \u001b[38;5;241m=\u001b[39m scaler_baseline\u001b[38;5;241m.\u001b[39minverse_transform(predictions_turn_baseline\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    409\u001b[0m     predictions_turn_baseline\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], predictions_turn_baseline\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m targets_turn_unscaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets_turn_current\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    413\u001b[0m     targets_turn_current\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], targets_turn_current\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m    416\u001b[0m history_inputs_unscaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(\n\u001b[1;32m    417\u001b[0m     inputs_turn_current[:, :, numeric_feature_indices]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n\u001b[1;32m    418\u001b[0m )\u001b[38;5;241m.\u001b[39mreshape(inputs_turn_current\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], inputs_turn_current\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n",
      "File \u001b[0;32m~/anaconda3/envs/drone_detection/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1088\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1085\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1087\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1088\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/anaconda3/envs/drone_detection/lib/python3.10/site-packages/sklearn/utils/validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/drone_detection/lib/python3.10/site-packages/sklearn/utils/_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[0;32m~/anaconda3/envs/drone_detection/lib/python3.10/site-packages/torch/_tensor.py:1085\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# \n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# \n",
    "path = Path('csv_out')\n",
    "eval_video_path = Path('eval_model_on_video')\n",
    "\n",
    "# \n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = eval_video_path / f'output_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Outputs will be saved to: {output_dir}')\n",
    "\n",
    "# \n",
    "df1 = pd.read_csv(path / 'tracking_data.csv')\n",
    "df2 = pd.read_csv(path / 'overall_turn_label.csv')\n",
    "\n",
    "# \n",
    "df_merged = pd.merge(df1, df2[['id', 'frame', 'overall_turn_label']], on=['id', 'frame'], how='left')\n",
    "\n",
    "# ID 'overall_turn_label' \n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='ffill')\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='bfill')\n",
    "\n",
    "# \n",
    "missing_values = df_merged['overall_turn_label'].isnull().sum()\n",
    "print(f\"Number of missing 'overall_turn_label': {missing_values}\")\n",
    "\n",
    "#  'straight'\n",
    "df_merged['overall_turn_label'] = df_merged['overall_turn_label'].fillna('straight')\n",
    "\n",
    "# One-Hot  'overall_turn_label'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turn_labels_encoded = encoder.fit_transform(df_merged[['overall_turn_label']])\n",
    "turn_label_columns = encoder.get_feature_names_out(['overall_turn_label'])\n",
    "df_merged[turn_label_columns] = turn_labels_encoded\n",
    "\n",
    "# \n",
    "input_features = ['center_x', 'center_y'] + list(turn_label_columns)\n",
    "\n",
    "# \n",
    "sequence_length = 90  # 9030fps3\n",
    "predict_length = 45   # 4530fps1.5\n",
    "\n",
    "# \n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "sequence_vehicle_ids = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  #  'center_x'  'center_y'\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "        sequence_vehicle_ids.append(track_id)\n",
    "\n",
    "#  NumPy \n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "sequence_vehicle_ids = np.array(sequence_vehicle_ids)\n",
    "\n",
    "# \n",
    "numeric_feature_indices = [0, 1]  # 'center_x'  'center_y' \n",
    "\n",
    "# \n",
    "all_numeric_inputs = input_sequences[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_numeric_inputs)\n",
    "\n",
    "# \n",
    "input_sequences[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs).reshape(\n",
    "    input_sequences.shape[0], input_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "all_numeric_targets = target_sequences.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences = scaler.transform(all_numeric_targets).reshape(\n",
    "    target_sequences.shape[0], target_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "input_features_baseline = ['center_x', 'center_y']\n",
    "\n",
    "input_sequences_baseline = []\n",
    "target_sequences_baseline = []\n",
    "sequence_vehicle_ids_baseline = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features_baseline].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]\n",
    "\n",
    "        input_sequences_baseline.append(input_seq)\n",
    "        target_sequences_baseline.append(target_seq)\n",
    "        sequence_vehicle_ids_baseline.append(track_id)\n",
    "\n",
    "#  NumPy \n",
    "input_sequences_baseline = np.array(input_sequences_baseline)\n",
    "target_sequences_baseline = np.array(target_sequences_baseline)\n",
    "sequence_vehicle_ids_baseline = np.array(sequence_vehicle_ids_baseline)\n",
    "\n",
    "# \n",
    "all_numeric_inputs_baseline = input_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler_baseline = StandardScaler()\n",
    "scaler_baseline.fit(all_numeric_inputs_baseline)\n",
    "\n",
    "# \n",
    "input_sequences_baseline = scaler_baseline.transform(all_numeric_inputs_baseline).reshape(\n",
    "    input_sequences_baseline.shape[0], input_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "all_numeric_targets_baseline = target_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_baseline = scaler_baseline.transform(all_numeric_targets_baseline).reshape(\n",
    "    target_sequences_baseline.shape[0], target_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=2):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        self.lstm_encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_decoder = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, target_len):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # \n",
    "        _, (hidden, cell) = self.lstm_encoder(x)\n",
    "\n",
    "        # \n",
    "        decoder_input = x[:, -1, :2].unsqueeze(1)  # \n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            out, (hidden, cell) = self.lstm_decoder(decoder_input, (hidden, cell))\n",
    "            out = self.fc_out(out)\n",
    "            outputs.append(out.squeeze(1))\n",
    "            decoder_input = out  # \n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# \n",
    "batch_size = 1024\n",
    "num_epochs = 10\n",
    "target_len = predict_length  # \n",
    "\n",
    "# \n",
    "input_size = input_sequences.shape[2]  # \n",
    "inputs = torch.tensor(input_sequences, dtype=torch.float32).to(device)\n",
    "targets = torch.tensor(target_sequences, dtype=torch.float32).to(device)\n",
    "model = TrajectoryPredictor(input_size=input_size, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "print(f'Number of batches per epoch: {len(data_loader)}')\n",
    "\n",
    "# \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for current model')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # \n",
    "        outputs = model(batch_inputs, target_len)\n",
    "\n",
    "        # \n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for current model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# \n",
    "input_size_baseline = input_sequences_baseline.shape[2]  # \n",
    "inputs_baseline = torch.tensor(input_sequences_baseline, dtype=torch.float32).to(device)\n",
    "targets_baseline = torch.tensor(target_sequences_baseline, dtype=torch.float32).to(device)\n",
    "model_baseline = TrajectoryPredictor(input_size=input_size_baseline, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset_baseline = TensorDataset(inputs_baseline, targets_baseline)\n",
    "data_loader_baseline = DataLoader(dataset_baseline, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples for baseline: {len(dataset_baseline)}')\n",
    "print(f'Number of batches per epoch for baseline: {len(data_loader_baseline)}')\n",
    "\n",
    "# \n",
    "criterion_baseline = nn.MSELoss()\n",
    "optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for baseline model')\n",
    "    model_baseline.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer_baseline.zero_grad()\n",
    "\n",
    "        # \n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "\n",
    "        # \n",
    "        loss = criterion_baseline(outputs, batch_targets)\n",
    "\n",
    "        # \n",
    "        loss.backward()\n",
    "        optimizer_baseline.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader_baseline)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for baseline model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# \n",
    "def compute_metrics(predictions, targets, horizons):\n",
    "    metrics = {}\n",
    "    for horizon in horizons:\n",
    "        outputs_at_horizon = predictions[:, :horizon, :]  # [num_samples, horizon, 2]\n",
    "        targets_at_horizon = targets[:, :horizon, :]\n",
    "\n",
    "        # \n",
    "        errors = outputs_at_horizon - targets_at_horizon  # [num_samples, horizon, 2]\n",
    "        squared_errors = errors ** 2\n",
    "        mse = squared_errors.mean().item()\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        abs_errors = errors.abs()\n",
    "        mae = abs_errors.mean().item()\n",
    "\n",
    "        #  ADE\n",
    "        displacement_errors = torch.norm(errors, dim=2)  #  x  y \n",
    "        ade = displacement_errors.mean().item()\n",
    "\n",
    "        #  FDE\n",
    "        final_errors = errors[:, -1, :]  # [num_samples, 2]\n",
    "        fde = torch.norm(final_errors, dim=1).mean().item()\n",
    "\n",
    "        metrics[horizon] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'ADE': ade,\n",
    "            'FDE': fde\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# \n",
    "horizons = {\n",
    "    15: 0.5,  # 0.5 15 \n",
    "    30: 1.0,  # 1.0 30 \n",
    "    45: 1.5   # 1.5 45 \n",
    "}\n",
    "\n",
    "#  predict_length \n",
    "horizons = {k: v for k, v in horizons.items() if k <= predict_length}\n",
    "\n",
    "# -----------------------------\n",
    "# \n",
    "# -----------------------------\n",
    "\n",
    "# 1. \n",
    "turn_labels = ['left_turn', 'right_turn']\n",
    "turning_df = df_merged[df_merged['overall_turn_label'].isin(turn_labels)]\n",
    "\n",
    "# 2. \n",
    "input_sequences_turn = []\n",
    "target_sequences_turn = []\n",
    "sequence_indices_turn = []\n",
    "\n",
    "grouped_turning = turning_df.groupby('id')\n",
    "\n",
    "for track_id, group in grouped_turning:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features_current = group[input_features].values  # \n",
    "    features_baseline = group[['center_x', 'center_y']].values  # \n",
    "\n",
    "    num_sequences = len(features_current) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq_current = features_current[i:i + sequence_length]\n",
    "        input_seq_baseline = features_baseline[i:i + sequence_length]\n",
    "        target_seq = features_current[i + sequence_length:i + sequence_length + predict_length, :2]  #  'center_x'  'center_y'\n",
    "\n",
    "        input_sequences_turn.append((input_seq_current, input_seq_baseline))\n",
    "        target_sequences_turn.append(target_seq)\n",
    "        sequence_indices_turn.append((track_id, i))\n",
    "\n",
    "#  NumPy \n",
    "input_sequences_turn_current = np.array([seq[0] for seq in input_sequences_turn])\n",
    "input_sequences_turn_baseline = np.array([seq[1] for seq in input_sequences_turn])\n",
    "target_sequences_turn = np.array(target_sequences_turn)\n",
    "\n",
    "# \n",
    "# \n",
    "all_numeric_inputs_turn_current = input_sequences_turn_current[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn_current[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs_turn_current).reshape(\n",
    "    input_sequences_turn_current.shape[0], input_sequences_turn_current.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "all_numeric_targets_turn = target_sequences_turn.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_turn_scaled = scaler.transform(all_numeric_targets_turn).reshape(\n",
    "    target_sequences_turn.shape[0], target_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "# \n",
    "all_numeric_inputs_turn_baseline = input_sequences_turn_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn_baseline = scaler_baseline.transform(all_numeric_inputs_turn_baseline).reshape(\n",
    "    input_sequences_turn_baseline.shape[0], input_sequences_turn_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "#  scaler \n",
    "target_sequences_turn_scaled_baseline = scaler_baseline.transform(all_numeric_targets_turn).reshape(\n",
    "    target_sequences_turn.shape[0], target_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "inputs_turn_current = torch.tensor(input_sequences_turn_current, dtype=torch.float32).to(device)\n",
    "inputs_turn_baseline = torch.tensor(input_sequences_turn_baseline, dtype=torch.float32).to(device)\n",
    "targets_turn_current = torch.tensor(target_sequences_turn_scaled, dtype=torch.float32).to(device)\n",
    "targets_turn_baseline = torch.tensor(target_sequences_turn_scaled_baseline, dtype=torch.float32).to(device)\n",
    "\n",
    "# \n",
    "dataset_turn_current = TensorDataset(inputs_turn_current, targets_turn_current)\n",
    "dataset_turn_baseline = TensorDataset(inputs_turn_baseline, targets_turn_baseline)\n",
    "\n",
    "data_loader_turn_current = DataLoader(dataset_turn_current, batch_size=batch_size, shuffle=False)\n",
    "data_loader_turn_baseline = DataLoader(dataset_turn_baseline, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# \n",
    "def get_predictions(model, data_loader, target_len):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, _ in data_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            outputs = model(batch_inputs, target_len)\n",
    "            predictions.append(outputs.cpu())\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    return predictions\n",
    "\n",
    "# \n",
    "predictions_turn_current = get_predictions(model, data_loader_turn_current, target_len)\n",
    "\n",
    "# \n",
    "predictions_turn_baseline = get_predictions(model_baseline, data_loader_turn_baseline, target_len)\n",
    "\n",
    "# \n",
    "# \n",
    "predictions_turn_current_unscaled = scaler.inverse_transform(predictions_turn_current.reshape(-1, 2)).reshape(\n",
    "    predictions_turn_current.shape[0], predictions_turn_current.shape[1], 2)\n",
    "\n",
    "# \n",
    "predictions_turn_baseline_unscaled = scaler_baseline.inverse_transform(predictions_turn_baseline.reshape(-1, 2)).reshape(\n",
    "    predictions_turn_baseline.shape[0], predictions_turn_baseline.shape[1], 2)\n",
    "\n",
    "# \n",
    "targets_turn_unscaled = scaler.inverse_transform(targets_turn_current.reshape(-1, 2)).reshape(\n",
    "    targets_turn_current.shape[0], targets_turn_current.shape[1], 2)\n",
    "\n",
    "# \n",
    "history_inputs_unscaled = scaler.inverse_transform(\n",
    "    inputs_turn_current[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    ").reshape(inputs_turn_current.shape[0], inputs_turn_current.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "def compute_trajectory_curvature(traj):\n",
    "    # traj: [seq_len, 2]\n",
    "    dx = np.gradient(traj[:, 0])\n",
    "    dy = np.gradient(traj[:, 1])\n",
    "    ddx = np.gradient(dx)\n",
    "    ddy = np.gradient(dy)\n",
    "    curvature = np.abs(dx * ddy - dy * ddx) / (dx * dx + dy * dy) ** 1.5\n",
    "    curvature[np.isnan(curvature)] = 0\n",
    "    curvature[np.isinf(curvature)] = 0\n",
    "    return curvature\n",
    "\n",
    "# \n",
    "num_samples = 5  # \n",
    "selected_indices = []\n",
    "\n",
    "for idx in range(predictions_turn_current_unscaled.shape[0]):\n",
    "    # \n",
    "    curvature_baseline = compute_trajectory_curvature(predictions_turn_baseline_unscaled[idx])\n",
    "    curvature_current = compute_trajectory_curvature(predictions_turn_current_unscaled[idx])\n",
    "\n",
    "    # \n",
    "    avg_curvature_baseline = np.mean(curvature_baseline)\n",
    "    avg_curvature_current = np.mean(curvature_current)\n",
    "\n",
    "    # \n",
    "    if avg_curvature_baseline < 0.001 and avg_curvature_current > avg_curvature_baseline + 0.005:\n",
    "        selected_indices.append(idx)\n",
    "\n",
    "    if len(selected_indices) >= num_samples:\n",
    "        break\n",
    "\n",
    "# \n",
    "for idx in selected_indices:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # \n",
    "    plt.plot(history_inputs_unscaled[idx, :, 0], history_inputs_unscaled[idx, :, 1], 'bo-', label='')\n",
    "\n",
    "    # \n",
    "    plt.plot(targets_turn_unscaled[idx, :, 0], targets_turn_unscaled[idx, :, 1], 'go-', label='')\n",
    "\n",
    "    # \n",
    "    plt.plot(predictions_turn_baseline_unscaled[idx, :, 0], predictions_turn_baseline_unscaled[idx, :, 1], 'ro--', label='')\n",
    "\n",
    "    # \n",
    "    plt.plot(predictions_turn_current_unscaled[idx, :, 0], predictions_turn_current_unscaled[idx, :, 1], 'co--', label='')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('center_x')\n",
    "    plt.ylabel('center_y')\n",
    "    vehicle_id, seq_idx = sequence_indices_turn[idx]\n",
    "    plt.title(f'ID {vehicle_id}  {seq_idx} ')\n",
    "\n",
    "    # \n",
    "    # plt.show()\n",
    "    # \n",
    "    figure_path = output_dir / f'Comparison_vehicle_{vehicle_id}_seq_{seq_idx}_{timestamp}.png'\n",
    "    plt.savefig(figure_path)\n",
    "    plt.close()\n",
    "    print(f'Plot saved to {figure_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4a2bc7-f8d0-491c-8155-28bcab9bbc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: eval_model_on_video/output_20241018_082709\n",
      "Number of missing 'overall_turn_label': 0\n",
      "Using device: cuda\n",
      "Total number of samples: 212539\n",
      "Number of batches per epoch: 416\n",
      "Starting epoch 1/10 for current model\n",
      "Epoch [1/10] for current model, Average Loss: 0.1269\n",
      "Starting epoch 2/10 for current model\n",
      "Epoch [2/10] for current model, Average Loss: 0.0067\n",
      "Starting epoch 3/10 for current model\n",
      "Epoch [3/10] for current model, Average Loss: 0.0046\n",
      "Starting epoch 4/10 for current model\n",
      "Epoch [4/10] for current model, Average Loss: 0.0043\n",
      "Starting epoch 5/10 for current model\n",
      "Epoch [5/10] for current model, Average Loss: 0.0218\n",
      "Starting epoch 6/10 for current model\n",
      "Epoch [6/10] for current model, Average Loss: 0.0047\n",
      "Starting epoch 7/10 for current model\n",
      "Epoch [7/10] for current model, Average Loss: 0.0033\n",
      "Starting epoch 8/10 for current model\n",
      "Epoch [8/10] for current model, Average Loss: 0.0028\n",
      "Starting epoch 9/10 for current model\n",
      "Epoch [9/10] for current model, Average Loss: 0.0033\n",
      "Starting epoch 10/10 for current model\n",
      "Epoch [10/10] for current model, Average Loss: 0.0026\n",
      "Total number of samples for baseline: 212539\n",
      "Number of batches per epoch for baseline: 416\n",
      "Starting epoch 1/10 for baseline model\n",
      "Epoch [1/10] for baseline model, Average Loss: 0.4205\n",
      "Starting epoch 2/10 for baseline model\n",
      "Epoch [2/10] for baseline model, Average Loss: 0.1072\n",
      "Starting epoch 3/10 for baseline model\n",
      "Epoch [3/10] for baseline model, Average Loss: 0.0280\n",
      "Starting epoch 4/10 for baseline model\n",
      "Epoch [4/10] for baseline model, Average Loss: 0.0098\n",
      "Starting epoch 5/10 for baseline model\n",
      "Epoch [5/10] for baseline model, Average Loss: 0.0065\n",
      "Starting epoch 6/10 for baseline model\n",
      "Epoch [6/10] for baseline model, Average Loss: 0.0049\n",
      "Starting epoch 7/10 for baseline model\n",
      "Epoch [7/10] for baseline model, Average Loss: 0.0043\n",
      "Starting epoch 8/10 for baseline model\n",
      "Epoch [8/10] for baseline model, Average Loss: 0.0051\n",
      "Starting epoch 9/10 for baseline model\n",
      "Epoch [9/10] for baseline model, Average Loss: 0.0082\n",
      "Starting epoch 10/10 for baseline model\n",
      "Epoch [10/10] for baseline model, Average Loss: 0.0062\n",
      "\n",
      "Metrics for current model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0285\n",
      "MAE: 0.0171\n",
      "ADE: 0.0274\n",
      "FDE: 0.0312\n",
      "\n",
      "Metrics for current model at horizon: 2.0 seconds (60 frames)\n",
      "RMSE: 0.0366\n",
      "MAE: 0.0208\n",
      "ADE: 0.0333\n",
      "FDE: 0.0486\n",
      "\n",
      "Metrics for current model at horizon: 3.0 seconds (90 frames)\n",
      "RMSE: 0.0521\n",
      "MAE: 0.0268\n",
      "ADE: 0.0426\n",
      "FDE: 0.0738\n",
      "\n",
      "Metrics for baseline model at horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0336\n",
      "MAE: 0.0221\n",
      "ADE: 0.0352\n",
      "FDE: 0.0366\n",
      "\n",
      "Metrics for baseline model at horizon: 2.0 seconds (60 frames)\n",
      "RMSE: 0.0441\n",
      "MAE: 0.0241\n",
      "ADE: 0.0387\n",
      "FDE: 0.0478\n",
      "\n",
      "Metrics for baseline model at horizon: 3.0 seconds (90 frames)\n",
      "RMSE: 0.0605\n",
      "MAE: 0.0275\n",
      "ADE: 0.0444\n",
      "FDE: 0.0647\n",
      "Current model saved to eval_model_on_video/output_20241018_082709/trajectory_predictor_current_20241018_082709.pth\n",
      "Current scaler saved to eval_model_on_video/output_20241018_082709/scaler_current_20241018_082709.save\n",
      "Baseline model saved to eval_model_on_video/output_20241018_082709/trajectory_predictor_baseline_20241018_082709.pth\n",
      "Baseline scaler saved to eval_model_on_video/output_20241018_082709/scaler_baseline_20241018_082709.save\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'target_sequences_turn_baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 483\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m#  scaler \u001b[39;00m\n\u001b[1;32m    481\u001b[0m all_numeric_targets_turn_baseline \u001b[38;5;241m=\u001b[39m target_sequences_turn\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n\u001b[1;32m    482\u001b[0m target_sequences_turn_baseline \u001b[38;5;241m=\u001b[39m scaler_baseline\u001b[38;5;241m.\u001b[39mtransform(all_numeric_targets_turn_baseline)\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m--> 483\u001b[0m     \u001b[43mtarget_sequences_turn_baseline\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], target_sequences_turn_baseline\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mlen\u001b[39m(numeric_feature_indices))\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m    486\u001b[0m inputs_turn_baseline \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_sequences_turn_baseline, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_sequences_turn_baseline' is not defined"
     ]
    }
   ],
   "source": [
    "# \n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# \n",
    "path = Path('csv_out')\n",
    "eval_video_path = Path('eval_model_on_video')\n",
    "\n",
    "# \n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = eval_video_path / f'output_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Outputs will be saved to: {output_dir}')\n",
    "\n",
    "# \n",
    "df1 = pd.read_csv(path / 'tracking_data.csv')\n",
    "df2 = pd.read_csv(path / 'overall_turn_label.csv')\n",
    "\n",
    "# \n",
    "df_merged = pd.merge(df1, df2[['id', 'frame', 'overall_turn_label']], on=['id', 'frame'], how='left')\n",
    "\n",
    "# ID 'overall_turn_label' \n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='ffill')\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='bfill')\n",
    "\n",
    "# \n",
    "missing_values = df_merged['overall_turn_label'].isnull().sum()\n",
    "print(f\"Number of missing 'overall_turn_label': {missing_values}\")\n",
    "\n",
    "#  'straight'\n",
    "df_merged['overall_turn_label'] = df_merged['overall_turn_label'].fillna('straight')\n",
    "\n",
    "# One-Hot  'overall_turn_label'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turn_labels_encoded = encoder.fit_transform(df_merged[['overall_turn_label']])\n",
    "turn_label_columns = encoder.get_feature_names_out(['overall_turn_label'])\n",
    "df_merged[turn_label_columns] = turn_labels_encoded\n",
    "\n",
    "# \n",
    "input_features = ['center_x', 'center_y'] + list(turn_label_columns)\n",
    "\n",
    "# \n",
    "sequence_length = 180  # 18030fps6\n",
    "predict_length = 90    # 9030fps3\n",
    "\n",
    "# \n",
    "horizons = {\n",
    "    30: 1.0,  # 1.0 30 \n",
    "    60: 2.0,  # 2.0 60 \n",
    "    90: 3.0   # 3.0 90 \n",
    "}\n",
    "\n",
    "# \n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "sequence_vehicle_ids = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  #  'center_x'  'center_y'\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "        sequence_vehicle_ids.append(track_id)\n",
    "\n",
    "#  NumPy \n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "sequence_vehicle_ids = np.array(sequence_vehicle_ids)\n",
    "\n",
    "# \n",
    "numeric_feature_indices = [0, 1]  # 'center_x'  'center_y' \n",
    "\n",
    "# \n",
    "all_numeric_inputs = input_sequences[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_numeric_inputs)\n",
    "\n",
    "# \n",
    "input_sequences[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs).reshape(\n",
    "    input_sequences.shape[0], input_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "all_numeric_targets = target_sequences.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences = scaler.transform(all_numeric_targets).reshape(\n",
    "    target_sequences.shape[0], target_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "input_features_baseline = ['center_x', 'center_y']\n",
    "\n",
    "input_sequences_baseline = []\n",
    "target_sequences_baseline = []\n",
    "sequence_vehicle_ids_baseline = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features_baseline].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]\n",
    "\n",
    "        input_sequences_baseline.append(input_seq)\n",
    "        target_sequences_baseline.append(target_seq)\n",
    "        sequence_vehicle_ids_baseline.append(track_id)\n",
    "\n",
    "#  NumPy \n",
    "input_sequences_baseline = np.array(input_sequences_baseline)\n",
    "target_sequences_baseline = np.array(target_sequences_baseline)\n",
    "sequence_vehicle_ids_baseline = np.array(sequence_vehicle_ids_baseline)\n",
    "\n",
    "# \n",
    "all_numeric_inputs_baseline = input_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler_baseline = StandardScaler()\n",
    "scaler_baseline.fit(all_numeric_inputs_baseline)\n",
    "\n",
    "# \n",
    "input_sequences_baseline = scaler_baseline.transform(all_numeric_inputs_baseline).reshape(\n",
    "    input_sequences_baseline.shape[0], input_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "all_numeric_targets_baseline = target_sequences_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_baseline = scaler_baseline.transform(all_numeric_targets_baseline).reshape(\n",
    "    target_sequences_baseline.shape[0], target_sequences_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=2):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        self.lstm_encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_decoder = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, target_len):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # \n",
    "        _, (hidden, cell) = self.lstm_encoder(x)\n",
    "\n",
    "        # \n",
    "        decoder_input = x[:, -1, :2].unsqueeze(1)  # \n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            out, (hidden, cell) = self.lstm_decoder(decoder_input, (hidden, cell))\n",
    "            out = self.fc_out(out)\n",
    "            outputs.append(out.squeeze(1))\n",
    "            decoder_input = out  # \n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# \n",
    "batch_size = 512  # \n",
    "num_epochs = 10\n",
    "target_len = predict_length  # \n",
    "\n",
    "# \n",
    "input_size = input_sequences.shape[2]  # \n",
    "inputs = torch.tensor(input_sequences, dtype=torch.float32).to(device)\n",
    "targets = torch.tensor(target_sequences, dtype=torch.float32).to(device)\n",
    "model = TrajectoryPredictor(input_size=input_size, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "print(f'Number of batches per epoch: {len(data_loader)}')\n",
    "\n",
    "# \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for current model')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # \n",
    "        outputs = model(batch_inputs, target_len)\n",
    "\n",
    "        # \n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for current model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# \n",
    "input_size_baseline = input_sequences_baseline.shape[2]  # \n",
    "inputs_baseline = torch.tensor(input_sequences_baseline, dtype=torch.float32).to(device)\n",
    "targets_baseline = torch.tensor(target_sequences_baseline, dtype=torch.float32).to(device)\n",
    "model_baseline = TrajectoryPredictor(input_size=input_size_baseline, hidden_size=128, num_layers=2, output_size=2).to(device)\n",
    "\n",
    "dataset_baseline = TensorDataset(inputs_baseline, targets_baseline)\n",
    "data_loader_baseline = DataLoader(dataset_baseline, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples for baseline: {len(dataset_baseline)}')\n",
    "print(f'Number of batches per epoch for baseline: {len(data_loader_baseline)}')\n",
    "\n",
    "# \n",
    "criterion_baseline = nn.MSELoss()\n",
    "optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs} for baseline model')\n",
    "    model_baseline.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer_baseline.zero_grad()\n",
    "\n",
    "        # \n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "\n",
    "        # \n",
    "        loss = criterion_baseline(outputs, batch_targets)\n",
    "\n",
    "        # \n",
    "        loss.backward()\n",
    "        optimizer_baseline.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader_baseline)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] for baseline model, Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# \n",
    "def compute_metrics(predictions, targets, horizons):\n",
    "    metrics = {}\n",
    "    for horizon in horizons:\n",
    "        outputs_at_horizon = predictions[:, :horizon, :]  # [num_samples, horizon, 2]\n",
    "        targets_at_horizon = targets[:, :horizon, :]\n",
    "\n",
    "        # \n",
    "        errors = outputs_at_horizon - targets_at_horizon  # [num_samples, horizon, 2]\n",
    "        squared_errors = errors ** 2\n",
    "        mse = squared_errors.mean().item()\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        abs_errors = errors.abs()\n",
    "        mae = abs_errors.mean().item()\n",
    "\n",
    "        #  ADE\n",
    "        displacement_errors = torch.norm(errors, dim=2)  #  x  y \n",
    "        ade = displacement_errors.mean().item()\n",
    "\n",
    "        #  FDE\n",
    "        final_errors = errors[:, -1, :]  # [num_samples, 2]\n",
    "        fde = torch.norm(final_errors, dim=1).mean().item()\n",
    "\n",
    "        metrics[horizon] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'ADE': ade,\n",
    "            'FDE': fde\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs = []\n",
    "    total_targets = []\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "        total_outputs.append(outputs.cpu())\n",
    "        total_targets.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs = torch.cat(total_outputs, dim=0)\n",
    "    total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "    metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "\n",
    "    # \n",
    "    metrics_file = output_dir / 'metrics_current_model.txt'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f'Metrics for current model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "            f.write(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# \n",
    "model_baseline.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs_baseline = []\n",
    "    total_targets_baseline = []\n",
    "    for batch_inputs, batch_targets in data_loader_baseline:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model_baseline(batch_inputs, target_len)\n",
    "        total_outputs_baseline.append(outputs.cpu())\n",
    "        total_targets_baseline.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs_baseline = torch.cat(total_outputs_baseline, dim=0)\n",
    "    total_targets_baseline = torch.cat(total_targets_baseline, dim=0)\n",
    "\n",
    "    metrics_baseline = compute_metrics(total_outputs_baseline, total_targets_baseline, horizons.keys())\n",
    "\n",
    "    # \n",
    "    metrics_file_baseline = output_dir / 'metrics_baseline_model.txt'\n",
    "    with open(metrics_file_baseline, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f\"Metrics for baseline model at horizon: {time_sec} seconds ({horizon_frames} frames)\\n\")\n",
    "            f.write(f\"RMSE: {metrics_baseline[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics_baseline[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics_baseline[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics_baseline[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# \n",
    "model_file = output_dir / f'trajectory_predictor_current_{timestamp}.pth'\n",
    "scaler_file = output_dir / f'scaler_current_{timestamp}.save'\n",
    "\n",
    "torch.save(model.state_dict(), model_file)\n",
    "joblib.dump(scaler, scaler_file)\n",
    "print(f'Current model saved to {model_file}')\n",
    "print(f'Current scaler saved to {scaler_file}')\n",
    "\n",
    "model_file_baseline = output_dir / f'trajectory_predictor_baseline_{timestamp}.pth'\n",
    "scaler_file_baseline = output_dir / f'scaler_baseline_{timestamp}.save'\n",
    "\n",
    "torch.save(model_baseline.state_dict(), model_file_baseline)\n",
    "joblib.dump(scaler_baseline, scaler_file_baseline)\n",
    "print(f'Baseline model saved to {model_file_baseline}')\n",
    "print(f'Baseline scaler saved to {scaler_file_baseline}')\n",
    "\n",
    "# \n",
    "# ...\n",
    "\n",
    "# ---------------------------------\n",
    "# \n",
    "# ---------------------------------\n",
    "\n",
    "# 1. \n",
    "turn_labels = ['left_turn', 'right_turn']\n",
    "turning_df = df_merged[df_merged['overall_turn_label'].isin(turn_labels)]\n",
    "\n",
    "# 2. \n",
    "input_sequences_turn = []\n",
    "target_sequences_turn = []\n",
    "\n",
    "grouped_turning = turning_df.groupby('id')\n",
    "\n",
    "for track_id, group in grouped_turning:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  #  'center_x'  'center_y'\n",
    "\n",
    "        input_sequences_turn.append(input_seq)\n",
    "        target_sequences_turn.append(target_seq)\n",
    "\n",
    "#  NumPy \n",
    "input_sequences_turn = np.array(input_sequences_turn)\n",
    "target_sequences_turn = np.array(target_sequences_turn)\n",
    "\n",
    "#  scaler\n",
    "numeric_feature_indices = [0, 1]  # 'center_x', 'center_y'\n",
    "\n",
    "# \n",
    "all_numeric_inputs_turn = input_sequences_turn[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs_turn).reshape(\n",
    "    input_sequences_turn.shape[0], input_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "all_numeric_targets_turn = target_sequences_turn.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_turn = scaler.transform(all_numeric_targets_turn).reshape(\n",
    "    target_sequences_turn.shape[0], target_sequences_turn.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "inputs_turn = torch.tensor(input_sequences_turn, dtype=torch.float32).to(device)\n",
    "targets_turn = torch.tensor(target_sequences_turn, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset_turn = TensorDataset(inputs_turn, targets_turn)\n",
    "data_loader_turn = DataLoader(dataset_turn, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. \n",
    "def evaluate_model_on_turning(model, data_loader, target_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_outputs = []\n",
    "        total_targets = []\n",
    "        for batch_inputs, batch_targets in data_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            outputs = model(batch_inputs, target_len)\n",
    "            total_outputs.append(outputs.cpu())\n",
    "            total_targets.append(batch_targets.cpu())\n",
    "\n",
    "        total_outputs = torch.cat(total_outputs, dim=0)\n",
    "        total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "        metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "    return metrics\n",
    "\n",
    "metrics_turn_current = evaluate_model_on_turning(model, data_loader_turn, target_len)\n",
    "\n",
    "# 4. \n",
    "# \n",
    "input_sequences_turn_baseline = input_sequences_turn[:, :, :2]  #  'center_x'  'center_y'\n",
    "\n",
    "#  scaler \n",
    "all_numeric_inputs_turn_baseline = input_sequences_turn_baseline.reshape(-1, len(numeric_feature_indices))\n",
    "input_sequences_turn_baseline = scaler_baseline.transform(all_numeric_inputs_turn_baseline).reshape(\n",
    "    input_sequences_turn_baseline.shape[0], input_sequences_turn_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "#  scaler \n",
    "all_numeric_targets_turn_baseline = target_sequences_turn.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences_turn_baseline = scaler_baseline.transform(all_numeric_targets_turn_baseline).reshape(\n",
    "    target_sequences_turn_baseline.shape[0], target_sequences_turn_baseline.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# \n",
    "inputs_turn_baseline = torch.tensor(input_sequences_turn_baseline, dtype=torch.float32).to(device)\n",
    "targets_turn_baseline = torch.tensor(target_sequences_turn_baseline, dtype=torch.float32).to(device)\n",
    "\n",
    "dataset_turn_baseline = TensorDataset(inputs_turn_baseline, targets_turn_baseline)\n",
    "data_loader_turn_baseline = DataLoader(dataset_turn_baseline, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# \n",
    "metrics_turn_baseline = evaluate_model_on_turning(model_baseline, data_loader_turn_baseline, target_len)\n",
    "\n",
    "# 5. \n",
    "metrics_file_turn = output_dir / 'metrics_turning_vehicles.txt'\n",
    "with open(metrics_file_turn, 'w') as f:\n",
    "    for horizon_frames, time_sec in horizons.items():\n",
    "        print(f'\\nMetrics on turning vehicles at horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "        print(f\"Baseline Model:\")\n",
    "        print(f\"  RMSE: {metrics_turn_baseline[horizon_frames]['RMSE']:.4f}\")\n",
    "        print(f\"  MAE: {metrics_turn_baseline[horizon_frames]['MAE']:.4f}\")\n",
    "        print(f\"  ADE: {metrics_turn_baseline[horizon_frames]['ADE']:.4f}\")\n",
    "        print(f\"  FDE: {metrics_turn_baseline[horizon_frames]['FDE']:.4f}\")\n",
    "        print(f\"Current Model:\")\n",
    "        print(f\"  RMSE: {metrics_turn_current[horizon_frames]['RMSE']:.4f}\")\n",
    "        print(f\"  MAE: {metrics_turn_current[horizon_frames]['MAE']:.4f}\")\n",
    "        print(f\"  ADE: {metrics_turn_current[horizon_frames]['ADE']:.4f}\")\n",
    "        print(f\"  FDE: {metrics_turn_current[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "        f.write(f'Metrics on turning vehicles at horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "        f.write(f\"Baseline Model:\\n\")\n",
    "        f.write(f\"  RMSE: {metrics_turn_baseline[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "        f.write(f\"  MAE: {metrics_turn_baseline[horizon_frames]['MAE']:.4f}\\n\")\n",
    "        f.write(f\"  ADE: {metrics_turn_baseline[horizon_frames]['ADE']:.4f}\\n\")\n",
    "        f.write(f\"  FDE: {metrics_turn_baseline[horizon_frames]['FDE']:.4f}\\n\")\n",
    "        f.write(f\"Current Model:\\n\")\n",
    "        f.write(f\"  RMSE: {metrics_turn_current[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "        f.write(f\"  MAE: {metrics_turn_current[horizon_frames]['MAE']:.4f}\\n\")\n",
    "        f.write(f\"  ADE: {metrics_turn_current[horizon_frames]['ADE']:.4f}\\n\")\n",
    "        f.write(f\"  FDE: {metrics_turn_current[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "print(f\"Metrics on turning vehicles saved to {metrics_file_turn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89661b-e1be-4a5c-90bb-b631de538c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (drone_detection)",
   "language": "python",
   "name": "drone_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
