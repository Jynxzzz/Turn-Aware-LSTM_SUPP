{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb2dce2-8b68-48d9-a7b4-14418e9ef085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import warnings\n",
    "import datetime\n",
    "import joblib\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7344ad-4fda-4b08-8b7f-b69ee82e8f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be saved to: eval_model_on_video/output_20241018_072739\n",
      "缺失的 overall_turn_label 数量：0\n",
      "Using device: cuda\n",
      "Total number of samples: 239560\n",
      "Number of batches per epoch: 234\n",
      "Starting epoch 1/10\n",
      "Epoch [1/10], Batch [234/234], Loss: 0.0065\n",
      "Epoch [1/10], Average Loss: 0.1245\n",
      "Starting epoch 2/10\n",
      "Epoch [2/10], Batch [234/234], Loss: 0.0033\n",
      "Epoch [2/10], Average Loss: 0.0049\n",
      "Starting epoch 3/10\n",
      "Epoch [3/10], Batch [234/234], Loss: 0.0030\n",
      "Epoch [3/10], Average Loss: 0.0034\n",
      "Starting epoch 4/10\n",
      "Epoch [4/10], Batch [234/234], Loss: 0.0027\n",
      "Epoch [4/10], Average Loss: 0.0029\n",
      "Starting epoch 5/10\n",
      "Epoch [5/10], Batch [234/234], Loss: 0.0016\n",
      "Epoch [5/10], Average Loss: 0.0023\n",
      "Starting epoch 6/10\n",
      "Epoch [6/10], Batch [234/234], Loss: 0.0012\n",
      "Epoch [6/10], Average Loss: 0.0019\n",
      "Starting epoch 7/10\n",
      "Epoch [7/10], Batch [234/234], Loss: 0.0017\n",
      "Epoch [7/10], Average Loss: 0.0017\n",
      "Starting epoch 8/10\n",
      "Epoch [8/10], Batch [234/234], Loss: 0.0014\n",
      "Epoch [8/10], Average Loss: 0.0016\n",
      "Starting epoch 9/10\n",
      "Epoch [9/10], Batch [234/234], Loss: 0.0010\n",
      "Epoch [9/10], Average Loss: 0.0014\n",
      "Starting epoch 10/10\n",
      "Epoch [10/10], Batch [234/234], Loss: 0.0011\n",
      "Epoch [10/10], Average Loss: 0.0014\n",
      "\n",
      "Metrics for horizon: 0.5 seconds (15 frames)\n",
      "RMSE: 0.0230\n",
      "MAE: 0.0131\n",
      "ADE: 0.0212\n",
      "FDE: 0.0209\n",
      "\n",
      "Metrics for horizon: 1.0 seconds (30 frames)\n",
      "RMSE: 0.0234\n",
      "MAE: 0.0139\n",
      "ADE: 0.0227\n",
      "FDE: 0.0284\n",
      "\n",
      "Metrics for horizon: 1.5 seconds (45 frames)\n",
      "RMSE: 0.0306\n",
      "MAE: 0.0165\n",
      "ADE: 0.0272\n",
      "FDE: 0.0435\n",
      "Model saved to eval_model_on_video/output_20241018_072739/trajectory_predictor_20241018_072739.pth\n",
      "Scaler saved to eval_model_on_video/output_20241018_072739/scaler_20241018_072739.save\n",
      "Plot saved to eval_model_on_video/output_20241018_072739/vehicle_220_seq_186734_20241018_072739.png\n",
      "Plot saved to eval_model_on_video/output_20241018_072739/vehicle_238_seq_197244_20241018_072739.png\n",
      "Plot saved to eval_model_on_video/output_20241018_072739/vehicle_74_seq_133490_20241018_072739.png\n",
      "Plot saved to eval_model_on_video/output_20241018_072739/vehicle_93_seq_138163_20241018_072739.png\n",
      "Plot saved to eval_model_on_video/output_20241018_072739/vehicle_238_seq_196680_20241018_072739.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[2]:\n",
    "\n",
    "# 定义路径\n",
    "path = Path('csv_out')\n",
    "eval_video_path = Path('eval_model_on_video')\n",
    "\n",
    "# 创建新的保存目录，带有时间戳\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = eval_video_path / f'output_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Outputs will be saved to: {output_dir}')\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "# 读取数据\n",
    "df1 = pd.read_csv(path / 'tracking_data.csv')\n",
    "df2 = pd.read_csv(path / 'overall_turn_label.csv')\n",
    "\n",
    "# 合并数据帧\n",
    "df_merged = pd.merge(df1, df2[['id', 'frame', 'overall_turn_label']], on=['id', 'frame'], how='left')\n",
    "\n",
    "# 按 id 分组，对 overall_turn_label 进行前向和后向填充\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='ffill')\n",
    "df_merged['overall_turn_label'] = df_merged.groupby('id')['overall_turn_label'].fillna(method='bfill')\n",
    "\n",
    "# 检查是否仍有缺失值\n",
    "missing_values = df_merged['overall_turn_label'].isnull().sum()\n",
    "print(f\"缺失的 overall_turn_label 数量：{missing_values}\")\n",
    "\n",
    "# 如果仍有缺失值，可以选择填充默认值或删除这些行\n",
    "df_merged['overall_turn_label'] = df_merged['overall_turn_label'].fillna('straight')\n",
    "\n",
    "# 对 overall_turn_label 进行 One-Hot 编码\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "turn_labels_encoded = encoder.fit_transform(df_merged[['overall_turn_label']])\n",
    "turn_label_columns = encoder.get_feature_names_out(['overall_turn_label'])\n",
    "df_merged[turn_label_columns] = turn_labels_encoded\n",
    "\n",
    "# 定义输入特征\n",
    "input_features = ['center_x', 'center_y'] + list(turn_label_columns)\n",
    "\n",
    "# 定义序列长度\n",
    "sequence_length = 90  # 输入序列长度（90帧，相当于3秒的历史数据）\n",
    "predict_length = 45   # 输出序列长度（45帧，相当于1.5秒的预测）\n",
    "\n",
    "# 生成输入和目标序列\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "sequence_vehicle_ids = []\n",
    "\n",
    "grouped = df_merged.groupby('id')\n",
    "\n",
    "for track_id, group in grouped:\n",
    "    group = group.sort_values('frame').reset_index(drop=True)\n",
    "    features = group[input_features].values\n",
    "\n",
    "    num_sequences = len(features) - sequence_length - predict_length + 1\n",
    "    if num_sequences <= 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        input_seq = features[i:i + sequence_length]\n",
    "        target_seq = features[i + sequence_length:i + sequence_length + predict_length, :2]  # 只取 center_x 和 center_y\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "        sequence_vehicle_ids.append(track_id)\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "sequence_vehicle_ids = np.array(sequence_vehicle_ids)\n",
    "\n",
    "# 数据标准化\n",
    "numeric_feature_indices = [0, 1]  # 'center_x', 'center_y'\n",
    "\n",
    "all_numeric_inputs = input_sequences[:, :, numeric_feature_indices].reshape(-1, len(numeric_feature_indices))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_numeric_inputs)\n",
    "\n",
    "input_sequences[:, :, numeric_feature_indices] = scaler.transform(all_numeric_inputs).reshape(input_sequences.shape[0], input_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "all_numeric_targets = target_sequences.reshape(-1, len(numeric_feature_indices))\n",
    "target_sequences = scaler.transform(all_numeric_targets).reshape(target_sequences.shape[0], target_sequences.shape[1], len(numeric_feature_indices))\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 模型定义\n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=2):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        self.lstm_encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_decoder = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, target_len):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Encoder：输入为完整的输入特征，包括位置和 One-Hot 编码的转弯标签\n",
    "        _, (hidden, cell) = self.lstm_encoder(x)\n",
    "\n",
    "        # Decoder inputs: 使用输入序列的最后一个位置坐标作为初始输入\n",
    "        decoder_input = x[:, -1, :2].unsqueeze(1)  # 只取 'center_x' 和 'center_y'\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            # Decoder step\n",
    "            out, (hidden, cell) = self.lstm_decoder(decoder_input, (hidden, cell))\n",
    "            out = self.fc_out(out)\n",
    "            outputs.append(out.squeeze(1))\n",
    "            decoder_input = out  # 下一时间步的输入为当前输出的位置坐标\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 初始化模型\n",
    "input_size = input_sequences.shape[2]  # 包括所有输入特征\n",
    "output_size = 2  # 只预测 'center_x' 和 'center_y'\n",
    "model = TrajectoryPredictor(input_size=input_size, hidden_size=128, num_layers=2, output_size=output_size)\n",
    "\n",
    "# 转换为张量并移动到设备上\n",
    "inputs = torch.tensor(input_sequences, dtype=torch.float32)\n",
    "targets = torch.tensor(target_sequences, dtype=torch.float32)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "batch_size = 1024\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "print(f'Number of batches per epoch: {len(data_loader)}')\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "target_len = predict_length  # 预测序列的长度\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 500 == 0 or (batch_idx + 1) == len(data_loader):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "# 定义计算指标的函数\n",
    "def compute_metrics(predictions, targets, horizons):\n",
    "    metrics = {}\n",
    "    for horizon in horizons:\n",
    "        outputs_at_horizon = predictions[:, :horizon, :]  # shape: [num_samples, horizon, 2]\n",
    "        targets_at_horizon = targets[:, :horizon, :]\n",
    "\n",
    "        # Compute errors\n",
    "        errors = outputs_at_horizon - targets_at_horizon  # shape: [num_samples, horizon, 2]\n",
    "        squared_errors = errors ** 2\n",
    "        mse = squared_errors.mean().item()\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        abs_errors = errors.abs()\n",
    "        mae = abs_errors.mean().item()\n",
    "\n",
    "        # Compute ADE\n",
    "        displacement_errors = torch.norm(errors, dim=2)  # Euclidean distance over x and y\n",
    "        ade = displacement_errors.mean().item()\n",
    "\n",
    "        # Compute FDE\n",
    "        final_errors = errors[:, -1, :]  # shape: [num_samples, 2]\n",
    "        fde = torch.norm(final_errors, dim=1).mean().item()\n",
    "\n",
    "        metrics[horizon] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'ADE': ade,\n",
    "            'FDE': fde\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "# 在训练集上评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_outputs = []\n",
    "    total_targets = []\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        outputs = model(batch_inputs, target_len)\n",
    "        total_outputs.append(outputs.cpu())\n",
    "        total_targets.append(batch_targets.cpu())\n",
    "\n",
    "    total_outputs = torch.cat(total_outputs, dim=0)\n",
    "    total_targets = torch.cat(total_targets, dim=0)\n",
    "\n",
    "    # 定义预测的时间地平线\n",
    "    horizons = {\n",
    "        15: 0.5,  # 0.5 seconds (15 frames)\n",
    "        30: 1.0,  # 1.0 seconds (30 frames)\n",
    "        45: 1.5   # 1.5 seconds (45 frames)\n",
    "    }\n",
    "\n",
    "    # 过滤超过预测长度的地平线\n",
    "    horizons = {k: v for k, v in horizons.items() if k <= predict_length}\n",
    "\n",
    "    metrics = compute_metrics(total_outputs, total_targets, horizons.keys())\n",
    "\n",
    "    # 将指标保存到文件\n",
    "    metrics_file = output_dir / 'metrics.txt'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        for horizon_frames, time_sec in horizons.items():\n",
    "            print(f'\\nMetrics for horizon: {time_sec} seconds ({horizon_frames} frames)')\n",
    "            print(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\")\n",
    "            print(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\")\n",
    "            print(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\")\n",
    "            print(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\")\n",
    "\n",
    "            f.write(f'Metrics for horizon: {time_sec} seconds ({horizon_frames} frames)\\n')\n",
    "            f.write(f\"RMSE: {metrics[horizon_frames]['RMSE']:.4f}\\n\")\n",
    "            f.write(f\"MAE: {metrics[horizon_frames]['MAE']:.4f}\\n\")\n",
    "            f.write(f\"ADE: {metrics[horizon_frames]['ADE']:.4f}\\n\")\n",
    "            f.write(f\"FDE: {metrics[horizon_frames]['FDE']:.4f}\\n\\n\")\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "# 保存模型和 scaler，使用时间戳避免覆盖\n",
    "model_file = output_dir / f'trajectory_predictor_{timestamp}.pth'\n",
    "scaler_file = output_dir / f'scaler_{timestamp}.save'\n",
    "\n",
    "torch.save(model.state_dict(), model_file)\n",
    "joblib.dump(scaler, scaler_file)\n",
    "print(f'Model saved to {model_file}')\n",
    "print(f'Scaler saved to {scaler_file}')\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "# 车辆 ID 映射到索引\n",
    "vehicle_ids_of_interest = [50, 328, 220, 46, 201, 238, 278, 185, 309, 303, 74, 93, 127, 203, 219, 210, 280, 390]\n",
    "vehicle_id_to_indices = {}\n",
    "\n",
    "for vehicle_id in vehicle_ids_of_interest:\n",
    "    indices = np.where(sequence_vehicle_ids == vehicle_id)[0]\n",
    "    if len(indices) > 0:\n",
    "        vehicle_id_to_indices[vehicle_id] = indices\n",
    "    else:\n",
    "        print(f\"Vehicle ID {vehicle_id} not found in the sequences.\")\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "def plot_background_img():\n",
    "    # 读取视频的第一帧\n",
    "    video_path = 'one_video/DJI_0007.mp4'  # 将 'your_video.mp4' 替换为你的实际文件名\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if ret:\n",
    "        # 将 BGR 图像转换为 RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 在 Matplotlib 中显示图像\n",
    "        plt.imshow(frame_rgb)\n",
    "        \n",
    "        # 绘制 scatter plot 叠加在图像上\n",
    "        # plt.scatter(df['center_x'], df['center_y'], color='pink', s=0.5)\n",
    "        \n",
    "        plt.axis('on')  # 如果你不想显示坐标轴\n",
    "        background_image_path = output_dir / f'background_{timestamp}.jpeg'\n",
    "        plt.savefig(background_image_path)\n",
    "        plt.close()\n",
    "        print(f'Background image saved to {background_image_path}')\n",
    "        \n",
    "    else:\n",
    "        print(\"Cannot read video, please check video directory\")\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "# 可视化预测结果\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Collect all indices from all vehicle IDs\n",
    "    all_indices = []\n",
    "    index_to_vehicle_id = {}\n",
    "    for vehicle_id, indices in vehicle_id_to_indices.items():\n",
    "        for idx in indices:\n",
    "            all_indices.append(idx)\n",
    "            index_to_vehicle_id[idx] = vehicle_id  # Map index to vehicle ID\n",
    "\n",
    "    # Randomly select 5 indices\n",
    "    num_samples = 5\n",
    "    if len(all_indices) >= num_samples:\n",
    "        selected_indices = random.sample(all_indices, num_samples)\n",
    "    else:\n",
    "        selected_indices = all_indices  # If less than 5 sequences are available\n",
    "\n",
    "    for idx in selected_indices:\n",
    "        test_input = inputs[idx].unsqueeze(0).to(device)\n",
    "        true_target = targets[idx].to(device)\n",
    "\n",
    "        # Perform prediction\n",
    "        predicted_output = model(test_input, target_len)\n",
    "\n",
    "        # Convert predictions and true targets to NumPy arrays\n",
    "        predicted_output = predicted_output.squeeze(0).cpu().numpy()\n",
    "        true_target = true_target.cpu().numpy()\n",
    "\n",
    "        # Get historical input data for visualization\n",
    "        history_input = test_input.squeeze(0).cpu().numpy()\n",
    "\n",
    "        # **Inverse scaling**\n",
    "        numeric_feature_indices = [0, 1]  # Indices of 'center_x' and 'center_y'\n",
    "\n",
    "        # Inverse transform historical inputs\n",
    "        history_input_numeric = history_input[:, numeric_feature_indices]\n",
    "        history_input_unscaled = scaler.inverse_transform(history_input_numeric)\n",
    "\n",
    "        # Inverse transform predicted outputs\n",
    "        predicted_output_unscaled = scaler.inverse_transform(predicted_output)\n",
    "\n",
    "        # Inverse transform true targets\n",
    "        true_target_unscaled = scaler.inverse_transform(true_target)\n",
    "\n",
    "        # **Visualization**\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        # Plot historical trajectory\n",
    "        plt.plot(history_input_unscaled[:, 0], history_input_unscaled[:, 1], 'bo-', label='Historical Trajectory')\n",
    "\n",
    "        # Plot true future trajectory\n",
    "        plt.plot(true_target_unscaled[:, 0], true_target_unscaled[:, 1], 'go-', label='True Future Trajectory')\n",
    "\n",
    "        # Plot predicted future trajectory\n",
    "        plt.plot(predicted_output_unscaled[:, 0], predicted_output_unscaled[:, 1], 'ro--', label='Predicted Future Trajectory')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel('center_x')\n",
    "        plt.ylabel('center_y')\n",
    "        plt.title(f'Vehicle {index_to_vehicle_id[idx]} Trajectory Prediction (Sequence Index {idx})')\n",
    "\n",
    "        # 保存图片，使用车辆ID和序列索引作为文件名的一部分\n",
    "        figure_path = output_dir / f'vehicle_{index_to_vehicle_id[idx]}_seq_{idx}_{timestamp}.png'\n",
    "        plt.savefig(figure_path)\n",
    "        plt.close()\n",
    "        print(f'Plot saved to {figure_path}')\n",
    "\n",
    "        # 如果需要绘制背景图像，可以取消注释以下行\n",
    "        # plot_background_img()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566ba43-caa2-44c2-a6b8-8db4fe337d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ca5c5-bfff-4c1c-841b-48ea2b55922e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388dc19c-61d1-4961-9d8d-46f2bfceda9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d29fd50-6043-43de-a75b-0e1b7858955b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983fc53-b3c2-4892-8a80-e3d104e57391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (drone_detection)",
   "language": "python",
   "name": "drone_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
